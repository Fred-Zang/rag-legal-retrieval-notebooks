# ğŸ§  RAG_Legal__retrieval_notebook â€” Support de lecture (dÃ©marche, benchmarks, roadmap)

<span style="color:#8B949E;">Repo fourni Ã  titre de <b>lecture</b> pour illustrer une dÃ©marche RAG (retrieval & Ã©valuation) dans un temps contraint.</span>

---

## ğŸ“Œ MÃ©morandum (Ã  lire en premier)

> <span style="color:#4EA1FF; font-weight:700;">Contexte</span>  
> Ce dÃ©pÃ´t est un **support de lecture** : il vise Ã  prÃ©senter clairement notre **mÃ©thode**, notre **progression** et nos **choix techniques** (retrieval, Ã©valuation, itÃ©rations) dans un dÃ©lai court (pÃ©riode de fin dâ€™annÃ©e / phase de prÃ©-Ã©change).

- <span style="color:#FFA657; font-weight:700;">DonnÃ©es non incluses</span> : le corpus dâ€™exploration est volumineux (~60 Mo) et **nâ€™est pas versionnÃ©** ici.  
- <span style="color:#FFA657; font-weight:700;">Source non alignÃ©e client</span> : les essais ont Ã©tÃ© rÃ©alisÃ©s sur un corpus de rÃ©fÃ©rence (type LÃ©gifrance) **uniquement pour prototyper la mÃ©thode**, et non sur un Ã©quivalent â€œLegiMarocâ€.  
- <span style="color:#7EE787; font-weight:700;">Objectif</span> : permettre une lecture rapide des raisonnements, hypothÃ¨ses, mÃ©triques, rÃ©sultats et limites.  
- <span style="color:#7EE787; font-weight:700;">Suite logique</span> : une version **rejouable** (et industrialisable) sera construite **sur le corpus client** (ou un corpus public strictement Ã©quivalent) dÃ¨s cadrage.

---

> <span style="color:#7EE787; font-weight:700;">Intention & implication</span>  
> Ce dÃ©pÃ´t a Ã©tÃ© construit en mode <b>speed-tests</b> sur une pÃ©riode trÃ¨s courte (fin dâ€™annÃ©e), avec un objectif clair : <b>illustrer notre implication</b> et notre maniÃ¨re dâ€™attaquer un projet RAG juridique (cadrage, itÃ©rations, mÃ©triques, enseignements) Ã  partir de la description de mission reÃ§ue.  
> <br/>
> <span style="color:#8B949E;">
> Une Ã©tude sÃ©rieuse dÃ©marre selon nous par lâ€™analyse du <b>vrai corpus client</b> (ou dâ€™un extrait reprÃ©sentatif) : formats, structure, mÃ©tadonnÃ©es, versions/dates, typologies, Ã©volutions â€” afin de dÃ©finir une stratÃ©gie dâ€™extraction/chunking robuste.  
> Ici, nous avons utilisÃ© un <b>extrait LÃ©gifrance</b> uniquement comme â€œmatiÃ¨reâ€ pour dÃ©rouler la roadmap et valider la mÃ©canique (retrieval + Ã©valuation), sans prÃ©tendre Ã  une qualitÃ© â€œdÃ©ploiementâ€ ni Ã  des rÃ©sultats optimaux sur ce corpus de substitution.
> </span>

---

## ğŸš€ Parcours de lecture rapide (5â€“10 minutes)

<span style="color:#8B949E;">
<b>Note de lecture :</b> certains notebooks dÃ©marrent volontairement â€œbrutâ€ (cellules/outputs) car ils proviennent de tests itÃ©ratifs.  
Pour une lecture confortable, repÃ©rer les titres et sections Markdown, puis dÃ©rouler dans lâ€™ordre ci-dessous.
</span>

1) ğŸ§­ **Roadmap & questions (fil conducteur)**  
â†’ [`notebooks/Z_Roadmap-Questions.ipynb`](./notebooks/Z_Roadmap-Questions.ipynb)  
<span style="color:#8B949E;">Notre fil conducteur posÃ© dÃ¨s le dÃ©but : Ã©tapes, hypothÃ¨ses, jalons, et questions de cadrage apparues au fil des tests.</span>

2) ğŸ§± **Analyse corpus (fondations RAG : structure/qualitÃ©/extraction/chunking â€” vision â€œproductionâ€)**  
â†’ [`notebooks/Z_analyse_corpus_juridique_icons.ipynb`](./notebooks/Z_analyse_corpus_juridique_icons.ipynb)  
<span style="color:#8B949E;">IdÃ©es dâ€™analyse fondamentale Ã  mener sur le <b>vrai dataset client</b> (non rÃ©alisÃ©e ici sur lâ€™extrait de substitution), pour sÃ©curiser lâ€™extraction/chunking et la maintenabilitÃ©.</span>

3) ğŸ§ª **Bilan technique des Ã©tapes 01 â†’ 10 (progression retrieval + mÃ©triques + enseignements)**  
â†’ [`notebooks/Z_bilan_scripts_1-10.ipynb`](./notebooks/Z_bilan_scripts_1-10.ipynb)  
<span style="color:#8B949E;">SynthÃ¨se â€œstep by stepâ€ : pourquoi chaque Ã©tape existe, ce que nous mesurons, ce que nous validons, et ce que cela implique pour une exÃ©cution sur le corpus client.</span>

Ensuite, si besoin : lecture dÃ©taillÃ©e de la sÃ©rie de scripts **01 â†’ 10** dans lâ€™ordre.

---

## âœ… Ce que ce dÃ©pÃ´t dÃ©montre

- ğŸ§­ Une approche **itÃ©rative et mesurable** : baseline â†’ amÃ©liorations â†’ comparaison â†’ conclusions.  
- ğŸ§± Un cadrage â€œRAG-readyâ€ : filtres, chunking, indexation, retrieval (BM25 / dense / hybride), **Ã©valuation**.  
- ğŸ§° Une sÃ©paration volontaire de briques **rÃ©utilisables** (modules annexes) pour itÃ©rer vite, proprement.  
- ğŸ§ª Une attention particuliÃ¨re Ã  la **qualitÃ© du corpus** (mÃ©tadonnÃ©es, versions, typologies, structure) en vue dâ€™une mise en production robuste.

---

## ğŸ“š SÃ©rie principale : notebooks 01 â†’ 10 (dans lâ€™ordre)

> Chaque notebook correspond Ã  une Ã©tape et reflÃ¨te le code testÃ© Ã  lâ€™instant T.  
> Les sorties/observations prÃ©sentes sont celles obtenues sur le corpus dâ€™exploration (non inclus).

1. 01 â€” **BM25 simple (baseline)**  
   â†’ [`notebooks/01_BM25_simple_presentation.ipynb`](./notebooks/01_BM25_simple_presentation.ipynb)

2. 02 â€” **Benchmark BM25 (protocole & mÃ©triques)**  
   â†’ [`notebooks/02_Benchmark_BM25_presentation.ipynb`](./notebooks/02_Benchmark_BM25_presentation.ipynb)

3. 03 â€” **BM25 avec filtres (contraintes mÃ©tier)**  
   â†’ [`notebooks/03_Benchmark_filtre_BM25_presentation.ipynb`](./notebooks/03_Benchmark_filtre_BM25_presentation.ipynb)

4. 04 â€” **Dense retrieval (embeddings)**  
   â†’ [`notebooks04_Dense_Retrieval_embeddings_presentation.ipynb`](./notebooks/04_Dense_Retrieval_embeddings_presentation.ipynb)

5. 05 â€” **BM25 + â€œquery understandingâ€ (expansion/enrichissement)**  
   â†’ [`notebooks/05_BM25_Query_Understanding_presentation.ipynb`](./notebooks/05_BM25_Query_Understanding_presentation.ipynb)

6. 06 â€” **BM25 filtrÃ© + query understanding**  
   â†’ [`notebooks/06_BM25_filtered_query_understanding_presentation.ipynb`](./notebooks/06_BM25_filtered_query_understanding_presentation.ipynb)

7. 07 â€” **Dense + query understanding**  
   â†’ [`notebooks/07_Dense_with_Query_Understanding_presentation.ipynb`](./notebooks/07_Dense_with_Query_Understanding_presentation.ipynb)

8. 08 â€” **Chunking XML â†’ chunks exploitables (prÃ©-indexation)**  
   â†’ [`notebooks/08_corpus_chunker_xml_presentation.ipynb`](./notebooks/08_corpus_chunker_xml_presentation.ipynb)

9. 09 â€” **Benchmark BM25 sur chunks (JSONL chunkÃ©)**  
   â†’ [`notebooks/09_Benchmark_BM25_on_JSONL_chunks_presentation.ipynb`](./notebooks/09_Benchmark_BM25_on_JSONL_chunks_presentation.ipynb)

10. 10 â€” **Hybride BM25 + Dense via RRF sur chunks (fusion de rankings)**  
   â†’ [`notebooks/10_Benchmark_Hybride_RRF_BM25_Dense_chunks_presentation.ipynb`](./notebooks/10_Benchmark_Hybride_RRF_BM25_Dense_chunks_presentation.ipynb)

---

## ğŸ§© Modules .py, .yml et jsonl construits pour le besoin des tests

> Ces fichiers `.py` ont Ã©tÃ© volontairement sÃ©parÃ©s comme briques annexes (rÃ©utilisables) pour itÃ©rer rapidement dans le temps imparti.

- [`modules_annexes/corpus_chunks_extrait.jsonl`](./modules_annexes/corpus_chunks_extrait.jsonl) â€” extrait **reprÃ©sentatif** (100 chunks) du corpus JSONL **chunkÃ©** : 1 ligne = 1 chunk avec `doc_id`, `text` et mÃ©tadonnÃ©es (utilisÃ© pour illustrer le format et faciliter la lecture ici sans publier le corpus complet ~13 180 lignes trop volumineux).
- [`modules_annexes/corpus_loader.py`](./modules_annexes/corpus_loader.py) â€” point dâ€™entrÃ©e unique de chargement du corpus (XML ou JSONL chunkÃ©) + filtre simple, sans logique retrieval.
- [`modules_annexes/benchmark_queries.py`](./modules_annexes/benchmark_queries.py) â€” jeu de requÃªtes â€œV1â€ (questions + intentions + mots-clÃ©s) servant dâ€™oracle simple et stable pour comparer les runs.
- [`modules_annexes/juridical_dictionary.yml`](./modules_annexes/juridical_dictionary.yml) â€” dictionnaire mÃ©tier versionnÃ© (intentions â†’ concepts/termes/codes/articles cibles) utilisÃ© pour normaliser le langage utilisateur.
- [`modules_annexes/metrics.py`](./modules_annexes/metrics.py) â€” mÃ©triques dâ€™Ã©valuation retrieval (Recall@k, MRR, nDCG@k) avec pertinence binaire basÃ©e sur mots-clÃ©s (oracle V1).
- [`modules_annexes/query_understanding.py`](./modules_annexes/query_understanding.py) â€” pipeline de â€œquery understandingâ€ sans LLM : dÃ©tection dâ€™intention via dictionnaire + enrichissement de requÃªte avant retrieval.
- [`modules_annexes/corpus_loader_jsonl.py`](./modules_annexes/corpus_loader_jsonl.py) â€” chargeur dÃ©diÃ© au corpus JSONL chunkÃ© (1 ligne = 1 chunk) en conservant doc_id/text + mÃ©tadonnÃ©es utiles.
- [`modules_annexes/benchmark_queries_v2.py`](./modules_annexes/benchmark_queries_v2.py) â€” benchmark â€œarticle-awareâ€ (oracle par `meta.num` via prÃ©fixes dâ€™articles) + fallback mots-clÃ©s si la mÃ©tadonnÃ©e manque.


---

## â™»ï¸ RejouabilitÃ© (volontairement hors scope ici)

<span style="color:#8B949E;">
Ce dÃ©pÃ´t nâ€™a pas vocation Ã  Ãªtre â€œone-click runnableâ€ Ã  ce stade, car :
</span>

- le corpus dâ€™exploration nâ€™est pas versionnÃ©,
- et les donnÃ©es utilisÃ©es ne sont pas lâ€™Ã©quivalent cible cÃ´tÃ© client.

Une version rejouable pourra Ãªtre mise en place selon les contraintes retenues :
- exÃ©cution locale (venv/conda),
- ou notebook Colab,
- ou conteneur (Docker) si besoin dâ€™un environnement stable,
dÃ¨s que le corpus client et le cadrage (schÃ©mas/versions/typologies) seront disponibles.

---

## ğŸ”’ Notes dâ€™anonymisation

Ce dÃ©pÃ´t est destinÃ© Ã  rester **anonyme** conformÃ©ment aux contraintes cÃ´tÃ© client :  
- pas de donnÃ©es, pas dâ€™identifiants, pas dâ€™informations sensibles,  
- pas de rÃ©fÃ©rences nominatives (personnes/projets/organisations) dans le contenu final.

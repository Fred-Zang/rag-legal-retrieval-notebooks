{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1adc9cbd",
   "metadata": {},
   "source": [
    "<span style=\"color:#8B949E;\">\n",
    "<b>Note de lecture</b> ‚Äî Notebook issu de tests it√©ratifs (‚Äúspeed-tests‚Äù).  \n",
    "Le corpus utilis√© ici est un <b>dataset de substitution</b> (non align√© client) uniquement pour valider la m√©canique (retrieval + √©valuation) et d√©rouler la roadmap.  \n",
    "Pour le chemin complet, suivre l‚Äôordre 01 ‚Üí 10 et lire en priorit√© les sections Markdown.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3a7d8f",
   "metadata": {},
   "source": [
    "# üß† Stage 7 ‚Äî BM25 + *Query Understanding* (requ√™te enrichie)\n",
    "\n",
    "**But :** comparer un **BM25 sur requ√™te brute** vs un **BM25 sur requ√™te enrichie m√©tier** (dictionnaire juridique), puis mesurer l'impact via **Recall@10**, **MRR** et **nDCG@10**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ √Ä avoir dans le projet\n",
    "- `corpus_loader.py` (ou package) ‚Üí fournit `documents`\n",
    "- `query_understanding.py` ‚Üí `process_user_query`, `load_juridical_dictionary`\n",
    "- `benchmark_queries.py` ‚Üí `benchmark_queries`\n",
    "- `juridical_dictionary.yml` ‚Üí dictionnaire m√©tier\n",
    "- (optionnel) `metrics.py` ‚Üí fonctions de m√©triques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1331c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# ‚ñ∂Ô∏è D√©pendances \"pip\" (uniquement celles qui sont externes au projet)\n",
    "# Note : les modules `corpus_loader`, `query_understanding`, `benchmark_queries`, `metrics`\n",
    "# sont suppos√©s √™tre des fichiers .py DU PROJET (donc pas installables via pip).\n",
    "\n",
    "import sys, os\n",
    "print(\"Python:\", sys.version)\n",
    "\n",
    "# Installation dans l'environnement Jupyter courant\n",
    "# (relance la cellule si besoin apr√®s un red√©marrage du kernel)\n",
    "%pip -q install rank_bm25 pyyaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f37400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook folder : d:\\-- Projet RAG Avocats --\\codes_python\\notebooks\n",
      "Project root    : d:\\-- Projet RAG Avocats --\\codes_python\\notebooks\n",
      "corpus_loader.py: True\n"
     ]
    }
   ],
   "source": [
    "# üîß Rendre importables les modules locaux du projet\n",
    "# Si notre notebook est dans un sous-dossier (ex: \"notebooks/\"), on ajoute la racine du projet au PYTHONPATH.\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Point de d√©part : dossier du notebook\n",
    "HERE = Path.cwd()\n",
    "\n",
    "# Cas fr√©quent : notebook dans \"notebooks/\" ‚Üí racine = parent\n",
    "# Ajuste si besoin (parent.parent, etc.)\n",
    "PROJECT_ROOT = HERE\n",
    "if (HERE / \"notebooks\").exists() and not (HERE / \"corpus_loader.py\").exists():\n",
    "    PROJECT_ROOT = HERE.parent\n",
    "\n",
    "# On ajoute au sys.path en priorit√© (index 0)\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(\"Notebook folder :\", HERE)\n",
    "print(\"Project root    :\", PROJECT_ROOT)\n",
    "print(\"corpus_loader.py:\", (PROJECT_ROOT / \"corpus_loader.py\").exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c971cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus brut charg√© : 4422 documents\n",
      "Corpus filtr√© 'Code du travail' : 882 documents\n"
     ]
    }
   ],
   "source": [
    "# üì¶ Imports\n",
    "from rank_bm25 import BM25Okapi\n",
    "import re\n",
    "\n",
    "# Imports \"locaux projet\" : on affiche une erreur claire si un fichier manque\n",
    "try:\n",
    "    from corpus_loader import documents\n",
    "except ModuleNotFoundError as e:\n",
    "    raise ModuleNotFoundError(\n",
    "        \"Impossible d'importer `corpus_loader`. \"\n",
    "        \"V√©rifie que `corpus_loader.py` est bien dans PROJECT_ROOT (ou ajuste la cellule sys.path).\"\n",
    "    ) from e\n",
    "\n",
    "try:\n",
    "    from query_understanding import process_user_query, load_juridical_dictionary\n",
    "except ModuleNotFoundError as e:\n",
    "    raise ModuleNotFoundError(\n",
    "        \"Impossible d'importer `query_understanding`. \"\n",
    "        \"V√©rifie que `query_understanding.py` est bien dans PROJECT_ROOT.\"\n",
    "    ) from e\n",
    "\n",
    "try:\n",
    "    from benchmark_queries import benchmark_queries\n",
    "except ModuleNotFoundError:\n",
    "    # Fallback : mini jeu de requ√™tes pour que le notebook tourne quand m√™me\n",
    "    benchmark_queries = [\n",
    "        {\"question\": \"Quelles sont les conditions de la garde √† vue ?\", \"relevant_keywords\": [\"garde\", \"vue\"]},\n",
    "        {\"question\": \"D√©lai de prescription en mati√®re civile\", \"relevant_keywords\": [\"prescription\", \"d√©lai\"]},\n",
    "    ]\n",
    "    print(\"‚ö†Ô∏è `benchmark_queries` introuvable ‚Üí fallback minimal utilis√© (pour d√©mo).\")\n",
    "\n",
    "# Metrics : soit un module local, soit un fallback minimal\n",
    "try:\n",
    "    from metrics import recall_at_k, reciprocal_rank, ndcg_at_k\n",
    "except ModuleNotFoundError:\n",
    "    print(\"‚ö†Ô∏è `metrics` introuvable ‚Üí fallback minimal utilis√© (pour d√©mo).\")\n",
    "\n",
    "    def _is_relevant(doc_text: str, relevant_keywords) -> bool:\n",
    "        t = (doc_text or \"\").lower()\n",
    "        return any((kw or \"\").lower() in t for kw in relevant_keywords)\n",
    "\n",
    "    def recall_at_k(results, relevant_keywords, k: int = 10) -> float:\n",
    "        topk = results[:k]\n",
    "        return 1.0 if any(_is_relevant(doc.get(\"text\", \"\"), relevant_keywords) for doc, _ in topk) else 0.0\n",
    "\n",
    "    def reciprocal_rank(results, relevant_keywords) -> float:\n",
    "        for i, (doc, _) in enumerate(results, start=1):\n",
    "            if _is_relevant(doc.get(\"text\", \"\"), relevant_keywords):\n",
    "                return 1.0 / i\n",
    "        return 0.0\n",
    "\n",
    "    def ndcg_at_k(results, relevant_keywords, k: int = 10) -> float:\n",
    "        # Gain binaire : 1 si doc pertinent, sinon 0\n",
    "        gains = [1.0 if _is_relevant(doc.get(\"text\", \"\"), relevant_keywords) else 0.0\n",
    "                 for doc, _ in results[:k]]\n",
    "\n",
    "        def dcg(vals):\n",
    "            s = 0.0\n",
    "            for i, v in enumerate(vals, start=1):\n",
    "                s += v / ( (i + 1) ** 0.5 )  # discount doux (d√©mo)\n",
    "            return s\n",
    "\n",
    "        ideal = sorted(gains, reverse=True)\n",
    "        denom = dcg(ideal)\n",
    "        return (dcg(gains) / denom) if denom > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d92a2c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dico charg√© ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# üìö Chargement du dictionnaire juridique\n",
    "# Assure-toi que `juridical_dictionary.yml` est bien pr√©sent √† la racine du projet.\n",
    "dictionary_path = \"juridical_dictionary.yml\"\n",
    "dictionary = load_juridical_dictionary(dictionary_path)\n",
    "print(\"Dico charg√© ‚úÖ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72a8d6f",
   "metadata": {},
   "source": [
    "## üî§ 1) Tokenisation (simple & robuste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c95e67e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['article', 'l', '123', '4', 'd√©lai', 'de', 'prescription']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text: str):\n",
    "    # Tokenisation simple : mots alphanum√©riques en minuscules\n",
    "    return re.findall(r\"\\b\\w+\\b\", (text or \"\").lower())\n",
    "\n",
    "# Petit test\n",
    "tokenize(\"Article L. 123-4 : d√©lai de prescription ?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452083ee",
   "metadata": {},
   "source": [
    "## üß± 2) Construction de l‚Äôindex BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0fdb559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 pr√™t ‚úÖ - nb docs: 882\n"
     ]
    }
   ],
   "source": [
    "# On tokenise les documents (attendu : documents = [{\"text\": ...}, ...])\n",
    "tokenized_docs = [tokenize(doc.get(\"text\", \"\")) for doc in documents]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "def bm25_search(query: str, k: int = 10):\n",
    "    # Calcule les scores BM25 et retourne les top-k\n",
    "    scores = bm25.get_scores(tokenize(query))\n",
    "    ranked = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)\n",
    "    return ranked[:k]\n",
    "\n",
    "print(\"BM25 pr√™t ‚úÖ - nb docs:\", len(documents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc1b45",
   "metadata": {},
   "source": [
    "## üß™ 3) √âvaluation : requ√™te brute vs requ√™te enrichie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e23ec7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Recall@10': 0.3333333333333333,\n",
       "  'MRR': 0.3333333333333333,\n",
       "  'nDCG@10': 0.3333333333333333},\n",
       " {'Recall@10': 0.3333333333333333,\n",
       "  'MRR': 0.3333333333333333,\n",
       "  'nDCG@10': 0.3333333333333333})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_bm25(use_query_understanding: bool = False, k: int = 10):\n",
    "    recall_scores = []\n",
    "    mrr_scores = []\n",
    "    ndcg_scores = []\n",
    "\n",
    "    for q in benchmark_queries:\n",
    "        query = q[\"question\"]\n",
    "\n",
    "        # Enrichissement \"m√©tier\"\n",
    "        if use_query_understanding:\n",
    "            enriched = process_user_query(query, dictionary)\n",
    "            query = enriched.get(\"enriched_query\", query)\n",
    "\n",
    "        results = bm25_search(query, k=k)\n",
    "\n",
    "        recall_scores.append(recall_at_k(results, q[\"relevant_keywords\"], k))\n",
    "        mrr_scores.append(reciprocal_rank(results, q[\"relevant_keywords\"]))\n",
    "        ndcg_scores.append(ndcg_at_k(results, q[\"relevant_keywords\"], k))\n",
    "\n",
    "    n = max(len(benchmark_queries), 1)\n",
    "    return {\n",
    "        f\"Recall@{k}\": sum(recall_scores) / n,\n",
    "        \"MRR\": sum(mrr_scores) / n,\n",
    "        f\"nDCG@{k}\": sum(ndcg_scores) / n,\n",
    "    }\n",
    "\n",
    "baseline = evaluate_bm25(use_query_understanding=False, k=10)\n",
    "enriched = evaluate_bm25(use_query_understanding=True, k=10)\n",
    "\n",
    "baseline, enriched\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd6262c",
   "metadata": {},
   "source": [
    "## üìä 4) Comparaison lisible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07faee05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Recall@10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MRR",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "nDCG@10",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "a13eee80-2f16-4184-94ec-553f5bbac5a9",
       "rows": [
        [
         "BM25 requ√™te brute",
         "0.3333333333333333",
         "0.3333333333333333",
         "0.3333333333333333"
        ],
        [
         "BM25 requ√™te enrichie",
         "0.3333333333333333",
         "0.3333333333333333",
         "0.3333333333333333"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>MRR</th>\n",
       "      <th>nDCG@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BM25 requ√™te brute</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BM25 requ√™te enrichie</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Recall@10       MRR   nDCG@10\n",
       "BM25 requ√™te brute      0.333333  0.333333  0.333333\n",
       "BM25 requ√™te enrichie   0.333333  0.333333  0.333333"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([baseline, enriched], index=[\"BM25 requ√™te brute\", \"BM25 requ√™te enrichie\"])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb62ad60",
   "metadata": {},
   "source": [
    "## üîé 5) Inspection qualitative sur une requ√™te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1e8b79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Requ√™te brute ===\n",
      "01  score=23.5373  |  (no id)\n",
      "02  score=12.7041  |  (no id)\n",
      "03  score=12.6815  |  (no id)\n",
      "04  score=12.6591  |  (no id)\n",
      "05  score=12.2068  |  (no id)\n",
      "\n",
      "=== Requ√™te enrichie ===\n",
      "Enriched query: Dans quels cas un CDI peut-il √™tre rompu sans pr√©avis ?\n",
      "01  score=23.5373  |  (no id)\n",
      "02  score=12.7041  |  (no id)\n",
      "03  score=12.6815  |  (no id)\n",
      "04  score=12.6591  |  (no id)\n",
      "05  score=12.2068  |  (no id)\n"
     ]
    }
   ],
   "source": [
    "# Modifier la requ√™te pour tester rapidement\n",
    "query = benchmark_queries[0][\"question\"]\n",
    "\n",
    "print(\"=== Requ√™te brute ===\")\n",
    "for i, (doc, score) in enumerate(bm25_search(query, k=5), start=1):\n",
    "    print(f\"{i:02d}  score={score:.4f}  |  {doc.get('id', '(no id)')}\")\n",
    "\n",
    "print(\"\\n=== Requ√™te enrichie ===\")\n",
    "enriched_q = process_user_query(query, dictionary).get(\"enriched_query\", query)\n",
    "print(\"Enriched query:\", enriched_q)\n",
    "\n",
    "for i, (doc, score) in enumerate(bm25_search(enriched_q, k=5), start=1):\n",
    "    print(f\"{i:02d}  score={score:.4f}  |  {doc.get('id', '(no id)')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e54e49d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Aucune intention m√©tier d√©tect√©e\n",
      "Dans quels cas un CDI peut-il √™tre rompu sans pr√©avis ?\n"
     ]
    }
   ],
   "source": [
    "res = process_user_query(query, dictionary)\n",
    "print(res[\"intent_detected\"]) # si None explique pourquoi requete ET requete enrichie donnent le m√™me r√©sultat\n",
    "print(res[\"notes\"] if \"notes\" in res else \"\")\n",
    "print(res[\"enriched_query\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461bce06",
   "metadata": {},
   "source": [
    "## üß© STAGE 5 ‚Äî BM25 + ‚ÄúQuery Understanding‚Äù : analyse finale (avec le diagnostic intention=None)\n",
    "\n",
    "### üßæ Donn√©es & p√©rim√®tre (inchang√©s)\n",
    "- Corpus brut : **4422** documents  \n",
    "- Corpus filtr√© **¬´ Code du travail ¬ª** : **882** documents  \n",
    "‚úÖ M√™me p√©rim√®tre que les stages pr√©c√©dents ‚Üí comparaison directe.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä R√©sultats (m√©triques)\n",
    "### BM25 ‚Äî requ√™te brute\n",
    "- Recall@10 = **0.333**\n",
    "- MRR = **0.333**\n",
    "- nDCG@10 = **0.333**\n",
    "\n",
    "### BM25 ‚Äî requ√™te enrichie ‚Äúm√©tier‚Äù\n",
    "- Recall@10 = **0.333**\n",
    "- MRR = **0.333**\n",
    "- nDCG@10 = **0.333**\n",
    "\n",
    "‚û°Ô∏è Lecture : aucune am√©lioration mesurable sur ce benchmark.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Diagnostic cl√© (pour expliquer pourquoi c‚Äôest identique)\n",
    "Sur la requ√™te test√©e, le module de ‚Äúquery understanding‚Äù renvoie :\n",
    "\n",
    "- `intent_detected = None`\n",
    "- message : **¬´ Aucune intention m√©tier d√©tect√©e ¬ª**\n",
    "- `enriched_query` = **requ√™te d‚Äôorigine** (inchang√©e)\n",
    "\n",
    "‚úÖ Cons√©quence directe : comme la requ√™te envoy√©e √† BM25 est **strictement la m√™me**,  \n",
    "le ranking et les scores BM25 sont **strictement identiques** :\n",
    "\n",
    "- Top-5 scores identiques (23.5373, 12.7041, 12.6815, 12.6591, 12.2068)\n",
    "- Donc aucune variation possible sur Recall@10 / MRR / nDCG@10.\n",
    "\n",
    "‚û°Ô∏è Conclusion importante : **l‚Äôabsence de gain ne refl√®te pas un √©chec de BM25**,  \n",
    "mais le fait que la couche ‚Äúm√©tier‚Äù **n‚Äôa pas √©t√© activ√©e** sur cette question.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Comparaison avec les stages pr√©c√©dents\n",
    "- **STAGE 3 (BM25 filtr√©)** : baseline = **0.333 / 0.333 / 0.333**\n",
    "- **STAGE 4 (Dense)** : **0.333 / 0.333 / 0.292** (ranking plus bruit√©)\n",
    "- **STAGE 5 (BM25 + enrichissement)** : **identique** √† STAGE 3, car `intent=None` ‚Üí pas d‚Äôenrichissement\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Conclusion\n",
    "√Ä ce stade, le STAGE 5 montre surtout un point ‚Äúing√©nierie‚Äù essentiel :\n",
    "\n",
    "> Tant que l‚Äôintention m√©tier n‚Äôest pas d√©tect√©e (`intent=None`),\n",
    "> la requ√™te enrichie = requ√™te brute,\n",
    "> donc aucune am√©lioration n‚Äôest possible.\n",
    "\n",
    "La prochaine √©tape logique (sans changer la logique BM25) serait de **renforcer la d√©tection d‚Äôintention** et/ou le dictionnaire, afin que l‚Äôenrichissement se d√©clenche r√©ellement et puisse √™tre √©valu√© objectivement.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML-Pro-venv)",
   "language": "python",
   "name": "ml-pro-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

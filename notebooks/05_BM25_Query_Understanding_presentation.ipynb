{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e3a7d8f",
   "metadata": {},
   "source": [
    "# ðŸ§  Stage 7 â€” BM25 + *Query Understanding* (requÃªte enrichie)\n",
    "\n",
    "**But :** comparer un **BM25 sur requÃªte brute** vs un **BM25 sur requÃªte enrichie mÃ©tier** (dictionnaire juridique), puis mesurer l'impact via **Recall@10**, **MRR** et **nDCG@10**.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Ã€ avoir dans le projet\n",
    "- `corpus_loader.py` (ou package) â†’ fournit `documents`\n",
    "- `query_understanding.py` â†’ `process_user_query`, `load_juridical_dictionary`\n",
    "- `benchmark_queries.py` â†’ `benchmark_queries`\n",
    "- `juridical_dictionary.yml` â†’ dictionnaire mÃ©tier\n",
    "- (optionnel) `metrics.py` â†’ fonctions de mÃ©triques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1331c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# â–¶ï¸ DÃ©pendances \"pip\" (uniquement celles qui sont externes au projet)\n",
    "# Note : les modules `corpus_loader`, `query_understanding`, `benchmark_queries`, `metrics`\n",
    "# sont supposÃ©s Ãªtre des fichiers .py DU PROJET (donc pas installables via pip).\n",
    "\n",
    "import sys, os\n",
    "print(\"Python:\", sys.version)\n",
    "\n",
    "# Installation dans l'environnement Jupyter courant\n",
    "# (relance la cellule si besoin aprÃ¨s un redÃ©marrage du kernel)\n",
    "%pip -q install rank_bm25 pyyaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f37400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook folder : d:\\-- Projet RAG Avocats --\\codes_python\\notebooks\n",
      "Project root    : d:\\-- Projet RAG Avocats --\\codes_python\\notebooks\n",
      "corpus_loader.py: True\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”§ Rendre importables les modules locaux du projet\n",
    "# Si notre notebook est dans un sous-dossier (ex: \"notebooks/\"), on ajoute la racine du projet au PYTHONPATH.\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Point de dÃ©part : dossier du notebook\n",
    "HERE = Path.cwd()\n",
    "\n",
    "# Cas frÃ©quent : notebook dans \"notebooks/\" â†’ racine = parent\n",
    "# Ajuste si besoin (parent.parent, etc.)\n",
    "PROJECT_ROOT = HERE\n",
    "if (HERE / \"notebooks\").exists() and not (HERE / \"corpus_loader.py\").exists():\n",
    "    PROJECT_ROOT = HERE.parent\n",
    "\n",
    "# On ajoute au sys.path en prioritÃ© (index 0)\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(\"Notebook folder :\", HERE)\n",
    "print(\"Project root    :\", PROJECT_ROOT)\n",
    "print(\"corpus_loader.py:\", (PROJECT_ROOT / \"corpus_loader.py\").exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c971cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus brut chargÃ© : 4422 documents\n",
      "Corpus filtrÃ© 'Code du travail' : 882 documents\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“¦ Imports\n",
    "from rank_bm25 import BM25Okapi\n",
    "import re\n",
    "\n",
    "# Imports \"locaux projet\" : on affiche une erreur claire si un fichier manque\n",
    "try:\n",
    "    from corpus_loader import documents\n",
    "except ModuleNotFoundError as e:\n",
    "    raise ModuleNotFoundError(\n",
    "        \"Impossible d'importer `corpus_loader`. \"\n",
    "        \"VÃ©rifie que `corpus_loader.py` est bien dans PROJECT_ROOT (ou ajuste la cellule sys.path).\"\n",
    "    ) from e\n",
    "\n",
    "try:\n",
    "    from query_understanding import process_user_query, load_juridical_dictionary\n",
    "except ModuleNotFoundError as e:\n",
    "    raise ModuleNotFoundError(\n",
    "        \"Impossible d'importer `query_understanding`. \"\n",
    "        \"VÃ©rifie que `query_understanding.py` est bien dans PROJECT_ROOT.\"\n",
    "    ) from e\n",
    "\n",
    "try:\n",
    "    from benchmark_queries import benchmark_queries\n",
    "except ModuleNotFoundError:\n",
    "    # Fallback : mini jeu de requÃªtes pour que le notebook tourne quand mÃªme\n",
    "    benchmark_queries = [\n",
    "        {\"question\": \"Quelles sont les conditions de la garde Ã  vue ?\", \"relevant_keywords\": [\"garde\", \"vue\"]},\n",
    "        {\"question\": \"DÃ©lai de prescription en matiÃ¨re civile\", \"relevant_keywords\": [\"prescription\", \"dÃ©lai\"]},\n",
    "    ]\n",
    "    print(\"âš ï¸ `benchmark_queries` introuvable â†’ fallback minimal utilisÃ© (pour dÃ©mo).\")\n",
    "\n",
    "# Metrics : soit un module local, soit un fallback minimal\n",
    "try:\n",
    "    from metrics import recall_at_k, reciprocal_rank, ndcg_at_k\n",
    "except ModuleNotFoundError:\n",
    "    print(\"âš ï¸ `metrics` introuvable â†’ fallback minimal utilisÃ© (pour dÃ©mo).\")\n",
    "\n",
    "    def _is_relevant(doc_text: str, relevant_keywords) -> bool:\n",
    "        t = (doc_text or \"\").lower()\n",
    "        return any((kw or \"\").lower() in t for kw in relevant_keywords)\n",
    "\n",
    "    def recall_at_k(results, relevant_keywords, k: int = 10) -> float:\n",
    "        topk = results[:k]\n",
    "        return 1.0 if any(_is_relevant(doc.get(\"text\", \"\"), relevant_keywords) for doc, _ in topk) else 0.0\n",
    "\n",
    "    def reciprocal_rank(results, relevant_keywords) -> float:\n",
    "        for i, (doc, _) in enumerate(results, start=1):\n",
    "            if _is_relevant(doc.get(\"text\", \"\"), relevant_keywords):\n",
    "                return 1.0 / i\n",
    "        return 0.0\n",
    "\n",
    "    def ndcg_at_k(results, relevant_keywords, k: int = 10) -> float:\n",
    "        # Gain binaire : 1 si doc pertinent, sinon 0\n",
    "        gains = [1.0 if _is_relevant(doc.get(\"text\", \"\"), relevant_keywords) else 0.0\n",
    "                 for doc, _ in results[:k]]\n",
    "\n",
    "        def dcg(vals):\n",
    "            s = 0.0\n",
    "            for i, v in enumerate(vals, start=1):\n",
    "                s += v / ( (i + 1) ** 0.5 )  # discount doux (dÃ©mo)\n",
    "            return s\n",
    "\n",
    "        ideal = sorted(gains, reverse=True)\n",
    "        denom = dcg(ideal)\n",
    "        return (dcg(gains) / denom) if denom > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d92a2c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dico chargÃ© âœ…\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“š Chargement du dictionnaire juridique\n",
    "# Assure-toi que `juridical_dictionary.yml` est bien prÃ©sent Ã  la racine du projet.\n",
    "dictionary_path = \"juridical_dictionary.yml\"\n",
    "dictionary = load_juridical_dictionary(dictionary_path)\n",
    "print(\"Dico chargÃ© âœ…\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72a8d6f",
   "metadata": {},
   "source": [
    "## ðŸ”¤ 1) Tokenisation (simple & robuste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c95e67e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['article', 'l', '123', '4', 'dÃ©lai', 'de', 'prescription']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text: str):\n",
    "    # Tokenisation simple : mots alphanumÃ©riques en minuscules\n",
    "    return re.findall(r\"\\b\\w+\\b\", (text or \"\").lower())\n",
    "\n",
    "# Petit test\n",
    "tokenize(\"Article L. 123-4 : dÃ©lai de prescription ?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452083ee",
   "metadata": {},
   "source": [
    "## ðŸ§± 2) Construction de lâ€™index BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0fdb559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 prÃªt âœ… - nb docs: 882\n"
     ]
    }
   ],
   "source": [
    "# On tokenise les documents (attendu : documents = [{\"text\": ...}, ...])\n",
    "tokenized_docs = [tokenize(doc.get(\"text\", \"\")) for doc in documents]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "def bm25_search(query: str, k: int = 10):\n",
    "    # Calcule les scores BM25 et retourne les top-k\n",
    "    scores = bm25.get_scores(tokenize(query))\n",
    "    ranked = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)\n",
    "    return ranked[:k]\n",
    "\n",
    "print(\"BM25 prÃªt âœ… - nb docs:\", len(documents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc1b45",
   "metadata": {},
   "source": [
    "## ðŸ§ª 3) Ã‰valuation : requÃªte brute vs requÃªte enrichie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e23ec7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Recall@10': 0.3333333333333333,\n",
       "  'MRR': 0.3333333333333333,\n",
       "  'nDCG@10': 0.3333333333333333},\n",
       " {'Recall@10': 0.3333333333333333,\n",
       "  'MRR': 0.3333333333333333,\n",
       "  'nDCG@10': 0.3333333333333333})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_bm25(use_query_understanding: bool = False, k: int = 10):\n",
    "    recall_scores = []\n",
    "    mrr_scores = []\n",
    "    ndcg_scores = []\n",
    "\n",
    "    for q in benchmark_queries:\n",
    "        query = q[\"question\"]\n",
    "\n",
    "        # Enrichissement \"mÃ©tier\"\n",
    "        if use_query_understanding:\n",
    "            enriched = process_user_query(query, dictionary)\n",
    "            query = enriched.get(\"enriched_query\", query)\n",
    "\n",
    "        results = bm25_search(query, k=k)\n",
    "\n",
    "        recall_scores.append(recall_at_k(results, q[\"relevant_keywords\"], k))\n",
    "        mrr_scores.append(reciprocal_rank(results, q[\"relevant_keywords\"]))\n",
    "        ndcg_scores.append(ndcg_at_k(results, q[\"relevant_keywords\"], k))\n",
    "\n",
    "    n = max(len(benchmark_queries), 1)\n",
    "    return {\n",
    "        f\"Recall@{k}\": sum(recall_scores) / n,\n",
    "        \"MRR\": sum(mrr_scores) / n,\n",
    "        f\"nDCG@{k}\": sum(ndcg_scores) / n,\n",
    "    }\n",
    "\n",
    "baseline = evaluate_bm25(use_query_understanding=False, k=10)\n",
    "enriched = evaluate_bm25(use_query_understanding=True, k=10)\n",
    "\n",
    "baseline, enriched\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd6262c",
   "metadata": {},
   "source": [
    "## ðŸ“Š 4) Comparaison lisible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07faee05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Recall@10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MRR",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "nDCG@10",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "a13eee80-2f16-4184-94ec-553f5bbac5a9",
       "rows": [
        [
         "BM25 requÃªte brute",
         "0.3333333333333333",
         "0.3333333333333333",
         "0.3333333333333333"
        ],
        [
         "BM25 requÃªte enrichie",
         "0.3333333333333333",
         "0.3333333333333333",
         "0.3333333333333333"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>MRR</th>\n",
       "      <th>nDCG@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BM25 requÃªte brute</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BM25 requÃªte enrichie</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Recall@10       MRR   nDCG@10\n",
       "BM25 requÃªte brute      0.333333  0.333333  0.333333\n",
       "BM25 requÃªte enrichie   0.333333  0.333333  0.333333"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([baseline, enriched], index=[\"BM25 requÃªte brute\", \"BM25 requÃªte enrichie\"])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb62ad60",
   "metadata": {},
   "source": [
    "## ðŸ”Ž 5) Inspection qualitative sur une requÃªte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e8b79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RequÃªte brute ===\n",
      "01  score=23.5373  |  (no id)\n",
      "02  score=12.7041  |  (no id)\n",
      "03  score=12.6815  |  (no id)\n",
      "04  score=12.6591  |  (no id)\n",
      "05  score=12.2068  |  (no id)\n",
      "\n",
      "=== RequÃªte enrichie ===\n",
      "Enriched query: Dans quels cas un CDI peut-il Ãªtre rompu sans prÃ©avis ?\n",
      "01  score=23.5373  |  (no id)\n",
      "02  score=12.7041  |  (no id)\n",
      "03  score=12.6815  |  (no id)\n",
      "04  score=12.6591  |  (no id)\n",
      "05  score=12.2068  |  (no id)\n"
     ]
    }
   ],
   "source": [
    "# Modifier la requÃªte pour tester rapidement\n",
    "query = benchmark_queries[0][\"question\"]\n",
    "\n",
    "print(\"=== RequÃªte brute ===\")\n",
    "for i, (doc, score) in enumerate(bm25_search(query, k=5), start=1):\n",
    "    print(f\"{i:02d}  score={score:.4f}  |  {doc.get('id', '(no id)')}\")\n",
    "\n",
    "print(\"\\n=== RequÃªte enrichie ===\")\n",
    "enriched_q = process_user_query(query, dictionary).get(\"enriched_query\", query)\n",
    "print(\"Enriched query:\", enriched_q)\n",
    "\n",
    "for i, (doc, score) in enumerate(bm25_search(enriched_q, k=5), start=1):\n",
    "    print(f\"{i:02d}  score={score:.4f}  |  {doc.get('id', '(no id)')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54e49d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Aucune intention mÃ©tier dÃ©tectÃ©e\n",
      "Dans quels cas un CDI peut-il Ãªtre rompu sans prÃ©avis ?\n"
     ]
    }
   ],
   "source": [
    "res = process_user_query(query, dictionary)\n",
    "print(res[\"intent_detected\"]) # si None explique pourquoi requete ET requete enrichie donnent le mÃªme rÃ©sultat\n",
    "print(res[\"notes\"] if \"notes\" in res else \"\")\n",
    "print(res[\"enriched_query\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461bce06",
   "metadata": {},
   "source": [
    "## ðŸ§© STAGE 5 â€” BM25 + â€œQuery Understandingâ€ : analyse finale (avec le diagnostic intention=None)\n",
    "\n",
    "### ðŸ§¾ DonnÃ©es & pÃ©rimÃ¨tre (inchangÃ©s)\n",
    "- Corpus brut : **4422** documents  \n",
    "- Corpus filtrÃ© **Â« Code du travail Â»** : **882** documents  \n",
    "âœ… MÃªme pÃ©rimÃ¨tre que les stages prÃ©cÃ©dents â†’ comparaison directe.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š RÃ©sultats (mÃ©triques)\n",
    "### BM25 â€” requÃªte brute\n",
    "- Recall@10 = **0.333**\n",
    "- MRR = **0.333**\n",
    "- nDCG@10 = **0.333**\n",
    "\n",
    "### BM25 â€” requÃªte enrichie â€œmÃ©tierâ€\n",
    "- Recall@10 = **0.333**\n",
    "- MRR = **0.333**\n",
    "- nDCG@10 = **0.333**\n",
    "\n",
    "âž¡ï¸ Lecture : aucune amÃ©lioration mesurable sur ce benchmark.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Diagnostic clÃ© (pour expliquer pourquoi câ€™est identique)\n",
    "Sur la requÃªte testÃ©e, le module de â€œquery understandingâ€ renvoie :\n",
    "\n",
    "- `intent_detected = None`\n",
    "- message : **Â« Aucune intention mÃ©tier dÃ©tectÃ©e Â»**\n",
    "- `enriched_query` = **requÃªte dâ€™origine** (inchangÃ©e)\n",
    "\n",
    "âœ… ConsÃ©quence directe : comme la requÃªte envoyÃ©e Ã  BM25 est **strictement la mÃªme**,  \n",
    "le ranking et les scores BM25 sont **strictement identiques** :\n",
    "\n",
    "- Top-5 scores identiques (23.5373, 12.7041, 12.6815, 12.6591, 12.2068)\n",
    "- Donc aucune variation possible sur Recall@10 / MRR / nDCG@10.\n",
    "\n",
    "âž¡ï¸ Conclusion importante : **lâ€™absence de gain ne reflÃ¨te pas un Ã©chec de BM25**,  \n",
    "mais le fait que la couche â€œmÃ©tierâ€ **nâ€™a pas Ã©tÃ© activÃ©e** sur cette question.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Comparaison avec les stages prÃ©cÃ©dents\n",
    "- **STAGE 3 (BM25 filtrÃ©)** : baseline = **0.333 / 0.333 / 0.333**\n",
    "- **STAGE 4 (Dense)** : **0.333 / 0.333 / 0.292** (ranking plus bruitÃ©)\n",
    "- **STAGE 5 (BM25 + enrichissement)** : **identique** Ã  STAGE 3, car `intent=None` â†’ pas dâ€™enrichissement\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Conclusion\n",
    "Ã€ ce stade, le STAGE 5 montre surtout un point â€œingÃ©nierieâ€ essentiel :\n",
    "\n",
    "> Tant que lâ€™intention mÃ©tier nâ€™est pas dÃ©tectÃ©e (`intent=None`),\n",
    "> la requÃªte enrichie = requÃªte brute,\n",
    "> donc aucune amÃ©lioration nâ€™est possible.\n",
    "\n",
    "La prochaine Ã©tape logique (sans changer la logique BM25) serait de **renforcer la dÃ©tection dâ€™intention** et/ou le dictionnaire, afin que lâ€™enrichissement se dÃ©clenche rÃ©ellement et puisse Ãªtre Ã©valuÃ© objectivement.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML-Pro-venv)",
   "language": "python",
   "name": "ml-pro-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

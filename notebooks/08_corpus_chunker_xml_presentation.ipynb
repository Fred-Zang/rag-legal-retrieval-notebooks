{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee855e18",
   "metadata": {},
   "source": [
    "<span style=\"color:#8B949E;\">\n",
    "<b>Note de lecture</b> ‚Äî Notebook issu de tests it√©ratifs (‚Äúspeed-tests‚Äù).  \n",
    "Le corpus utilis√© ici est un <b>dataset de substitution</b> (non align√© client) uniquement pour valider la m√©canique (retrieval + √©valuation) et d√©rouler la roadmap.  \n",
    "Pour le chemin complet, suivre l‚Äôordre 01 ‚Üí 10 et lire en priorit√© les sections Markdown.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6d4458",
   "metadata": {},
   "source": [
    "# üß± STAGE 8 ‚Äî Extraction + chunking ‚Äúpassage-level‚Äù (XML juridiques)\n",
    "\n",
    "Ce stage introduit la **brique manquante** observ√©e dans les stages pr√©c√©dents (BM25 / dense / hybride) :  \n",
    "‚û°Ô∏è **la granularit√© documentaire**.\n",
    "\n",
    "Jusqu‚Äôici, on benchmarkait des retrievers sur un corpus o√π la structure ‚ÄúXML concat√©n√©‚Äù pouvait :\n",
    "- injecter beaucoup de **bruit** (liens, historiques, m√©tadonn√©es, en-t√™tes techniques),\n",
    "- diluer le **signal juridique**,\n",
    "- et faire √©chouer des requ√™tes pourtant simples (top-k rempli de non-pertinents).\n",
    "\n",
    "Ce script construit un corpus **chunk√© au niveau ‚Äúpassage‚Äù** (paragraphes regroup√©s + d√©coupe phrase-aware), pr√™t √† √™tre index√© et benchmark√© √† l‚Äôidentique dans les stages suivants.\n",
    "\n",
    "---\n",
    "\n",
    "SCRIPT 8 ‚Äî EXTRACTION + CHUNKING \"PASSAGE-LEVEL\" POUR XML JURIDIQUES\n",
    "\n",
    "But du script\n",
    "-------------\n",
    "Ce script transforme un corpus de documents XML juridiques (structure type L√©gifrance,\n",
    "ou toute structure proche) en une liste de \"chunks\" (passages) propres, pr√™ts √† √™tre index√©s\n",
    "(BM25, embeddings, hybride) et √©valu√©s via les scripts de benchmark existants.\n",
    "\n",
    "Pourquoi (probl√®me observ√©)\n",
    "---------------------------\n",
    "Dans les XML juridiques, une grande partie du contenu peut √™tre compos√©e de :\n",
    "- liens (r√©f√©rences, historiques, citations) : balises LIEN / ... et structures associ√©es\n",
    "- m√©tadonn√©es (dates, identifiants, hi√©rarchie, titres multiples)\n",
    "- en-t√™tes techniques concat√©n√©s (ID, URL, statut, etc.) sur certains exports\n",
    "\n",
    "Si on concat√®ne \"tout le texte du XML\" et qu'on chunk en \"1 document = 1 chunk\",\n",
    "on indexe beaucoup de bruit et on dilue le signal juridique.\n",
    "Le passage-level am√©liore la pr√©cision du retrieval (question ‚Üî passage pertinent)\n",
    "et facilite la citation/sour√ßage.\n",
    "\n",
    "Ce script applique donc des principes simples :\n",
    "1) Extraire prioritairement le \"texte utile\" (paragraphes/blocs textuels) en √©vitant les zones bruyantes.\n",
    "2) Chunker au niveau passage (regroupement de paragraphes) avec d√©coupe \"phrase-aware\" si n√©cessaire,\n",
    "   pour √©viter de couper au milieu d'une phrase.\n",
    "3) Ajouter une tra√ßabilit√© permettant de reconstituer l'ordre des passages d'un m√™me document.\n",
    "\n",
    "Comment (r√©sum√©)\n",
    "----------------\n",
    "- Parsing XML via xml.etree.ElementTree\n",
    "- Suppression des namespaces (robustesse des find/findall sur des XML h√©t√©rog√®nes)\n",
    "- Extraction de m√©tadonn√©es minimales (id, num, url, dates, titre si trouv√©)\n",
    "- Extraction de paragraphes :\n",
    "    * priorit√© √† BLOC_TEXTUEL/CONTENU//p (sinon strat√©gies de fallback contr√¥l√©es)\n",
    "- Chunking passage-level :\n",
    "    * regroupement de paragraphes\n",
    "    * d√©coupe \"phrase-aware\" (ponctuation) si un paragraphe est trop long\n",
    "- Nettoyage l√©ger :\n",
    "    * suppression d'en-t√™tes techniques en pr√©fixe (ID/URL/stats concat√©n√©s) sur certains documents\n",
    "- Sauvegarde au format JSONL (1 chunk par ligne)\n",
    "\n",
    "Sorties attendues\n",
    "-----------------\n",
    "Chaque chunk est un dict contenant :\n",
    "- doc_id        : chemin du fichier XML source\n",
    "- doc_type      : type simple d√©duit du chemin (article / section_ta / other) pour filtrer rapidement\n",
    "- chunk_index   : index (0..n) pour reconstituer l'ordre et r√©cup√©rer des chunks voisins (i-1, i, i+1)\n",
    "- chunk_id      : identifiant stable (hash) calcul√© sur le texte r√©ellement index√©\n",
    "- text          : texte du passage (nettoy√©, faible bruit)\n",
    "- meta          : m√©tadonn√©es (id article, num, url, dates, titre...)\n",
    "- links_count   : nombre de liens extraits (m√©tadonn√©es)\n",
    "- links_sample  : √©chantillon de liens pour audit (m√©tadonn√©es), non index√©\n",
    "\n",
    "Compatibilit√©\n",
    "-------------\n",
    "Pour une compatibilit√© directe avec les scripts existants :\n",
    "- les champs doc_id et text sont pr√©sents (chargement facile c√¥t√© BM25/dense)\n",
    "- meta / liens / doc_type peuvent √™tre ignor√©s dans un POC, mais utiles pour filtrer et sourcer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d70efd1",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Code (identique au script, adapt√© notebook)\n",
    "- Le code ci-dessous est celui du script 08.\n",
    "- Seule adaptation notebook : le bloc `if __name__ == \"__main__\"` est retir√© pour √©viter de stopper le kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e62e439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Iterable, List, Optional\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Nettoyage texte (petit, robuste)\n",
    "# ----------------------------\n",
    "\n",
    "_WHITESPACE_RE = re.compile(r\"\\s+\", flags=re.UNICODE)\n",
    "\n",
    "# ----------------------------\n",
    "# Nettoyage l√©ger des \"en-t√™tes\" techniques parfois pr√©sents au d√©but des articles\n",
    "# ----------------------------\n",
    "\n",
    "# 1) Pr√©fixes techniques fr√©quents dans certains exports : ID + code opaque + \"LEGI\"\n",
    "_RE_PREFIX_ID = re.compile(\n",
    "    r\"^\\s*(LEGI(?:ARTI|SCTA|TEXT)\\d{6,})\\s+[A-Z0-9X]{10,}\\s+LEGI\\s+\",\n",
    "    flags=re.IGNORECASE,\n",
    ")\n",
    "\n",
    "# 2) Pr√©fixe contenant un chemin relatif vers le XML (ex: article/LEGI/ARTI/...xml)\n",
    "_RE_PREFIX_PATH = re.compile(\n",
    "    r\"^\\s*(?:article|section_ta|texte|text)/LEGI/(?:ARTI|SCTA|TEXT)/[^ \\n\\r\\t]+?\\.xml\\s+\",\n",
    "    flags=re.IGNORECASE,\n",
    ")\n",
    "\n",
    "# 3) Bloc \"m√©tadonn√©es\" souvent concat√©n√© : \"Article Lxxx-xx MODIFIE 2000-.. 2003-.. AUTONOME ...\"\n",
    "_RE_PREFIX_ARTICLE_META = re.compile(\n",
    "    r\"^\\s*Article\\s+[A-Z]?\\s*\\d[\\w\\-\\.]*\\s+(?:VIGUEUR|MODIFIE|ABROGE|ABROG√â|PERIME|P√âRIM√â)?\\s*\"\n",
    "    r\"(?:\\d{4}-\\d{2}-\\d{2}\\s+)?(?:\\d{4}-\\d{2}-\\d{2}\\s+)?\"\n",
    "    r\"(?:AUTONOME\\s+)?\",\n",
    "    flags=re.IGNORECASE,\n",
    ")\n",
    "\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalise un texte : trim + espaces multiples.\"\"\"\n",
    "    return _WHITESPACE_RE.sub(\" \", text.strip())\n",
    "\n",
    "\n",
    "def strip_technical_header(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Retire les en-t√™tes techniques (ID/chemin/meta concat√©n√©s) observ√©s sur certains documents.\n",
    "\n",
    "    Le nettoyage est volontairement conservateur :\n",
    "    - il ne s'applique que sur le d√©but du texte\n",
    "    - il fait au plus quelques passes\n",
    "    \"\"\"\n",
    "    t = normalize_text(text)\n",
    "\n",
    "    # Passe 1 : ID + code opaque + LEGI\n",
    "    t = _RE_PREFIX_ID.sub(\"\", t)\n",
    "\n",
    "    # Passe 2 : chemin relatif XML\n",
    "    t = _RE_PREFIX_PATH.sub(\"\", t)\n",
    "\n",
    "    # Passe 3 : mini-bloc \"Article ... MODIFIE ... AUTONOME ...\"\n",
    "    t = _RE_PREFIX_ARTICLE_META.sub(\"\", t)\n",
    "\n",
    "    # Nettoyage final\n",
    "    return normalize_text(t)\n",
    "\n",
    "\n",
    "def safe_findtext(root: ET.Element, path: str) -> Optional[str]:\n",
    "    \"\"\"Retourne root.find(path).text normalis√© si pr√©sent, sinon None.\"\"\"\n",
    "    node = root.find(path)\n",
    "    if node is not None and node.text:\n",
    "        t = normalize_text(node.text)\n",
    "        return t if t else None\n",
    "    return None\n",
    "\n",
    "\n",
    "def iter_text_nodes(elem: ET.Element) -> Iterable[str]:\n",
    "    \"\"\"\n",
    "    It√®re sur les textes 'utiles' d'un √©l√©ment XML :\n",
    "    - elem.text et elem.tail, en normalisant.\n",
    "    \"\"\"\n",
    "    if elem.text:\n",
    "        t = normalize_text(elem.text)\n",
    "        if t:\n",
    "            yield t\n",
    "    if elem.tail:\n",
    "        t = normalize_text(elem.tail)\n",
    "        if t:\n",
    "            yield t\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Robustesse XML : suppression des namespaces\n",
    "# ----------------------------\n",
    "\n",
    "def strip_namespaces(root: ET.Element) -> None:\n",
    "    \"\"\"\n",
    "    Supprime les namespaces des tags XML pour permettre des findall(\".//TAG\") simples.\n",
    "\n",
    "    Exemple:\n",
    "      {http://...}BLOC_TEXTUEL -> BLOC_TEXTUEL\n",
    "    \"\"\"\n",
    "    for elem in root.iter():\n",
    "        if isinstance(elem.tag, str) and \"}\" in elem.tag:\n",
    "            elem.tag = elem.tag.rsplit(\"}\", 1)[-1]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Typage simple des documents (utile pour filtrer)\n",
    "# ----------------------------\n",
    "\n",
    "def infer_doc_type_from_path(xml_path: str) -> str:\n",
    "    \"\"\"\n",
    "    D√©duit un type de document √† partir du chemin.\n",
    "\n",
    "    Sert √† filtrer rapidement (ex: exclure section_ta si trop bruit√©).\n",
    "    \"\"\"\n",
    "    p = xml_path.replace(\"/\", \"\\\\\").lower()\n",
    "    if \"\\\\article\\\\\" in p or \"\\\\arti\\\\\" in p:\n",
    "        return \"article\"\n",
    "    if \"\\\\section_ta\\\\\" in p or \"\\\\scta\\\\\" in p:\n",
    "        return \"section_ta\"\n",
    "    return \"other\"\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# D√©coupe phrase-aware (ponctuation)\n",
    "# ----------------------------\n",
    "\n",
    "_ABBREV_TOKENS = {\n",
    "    # Abr√©viations fr√©quentes en juridique / admin\n",
    "    \"art\", \"al\", \"alin\", \"n\", \"no\", \"n¬∞\", \"m\", \"mme\", \"dr\", \"pr\", \"st\", \"ste\",\n",
    "    # Marques d'articles/sections (√©vite de couper apr√®s \"L.\" \"R.\" etc.)\n",
    "    \"l\", \"r\", \"d\", \"c\",\n",
    "}\n",
    "\n",
    "\n",
    "def _looks_like_abbrev_before_dot(text: str, dot_index: int) -> bool:\n",
    "    \"\"\"\n",
    "    D√©tecte si un '.' correspond probablement √† une abr√©viation (ex: 'L.' / 'art.').\n",
    "    \"\"\"\n",
    "    left = text[: dot_index + 1].rstrip()\n",
    "    m = re.search(r\"([A-Za-z√Ä-√ø]{1,6})\\.$\", left)\n",
    "    if not m:\n",
    "        return False\n",
    "\n",
    "    token = m.group(1).lower()\n",
    "\n",
    "    # Lettre unique : L. R. D. etc.\n",
    "    if len(token) == 1 and token.isalpha():\n",
    "        return True\n",
    "\n",
    "    return token in _ABBREV_TOKENS\n",
    "\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    D√©coupe un texte en \"phrases\" via . ! ? avec une heuristique l√©g√®re.\n",
    "\n",
    "    Objectif : √©viter de couper au milieu d'une phrase lors du chunking.\n",
    "    \"\"\"\n",
    "    text = normalize_text(text)\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    sentences: List[str] = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "\n",
    "    for i, ch in enumerate(text):\n",
    "        if ch not in \".!?\":\n",
    "            continue\n",
    "\n",
    "        # On ne coupe pas sur abr√©viations\n",
    "        if ch == \".\" and _looks_like_abbrev_before_dot(text, i):\n",
    "            continue\n",
    "\n",
    "        # Fin de texte\n",
    "        if i == n - 1:\n",
    "            tail = text[start:].strip()\n",
    "            if tail:\n",
    "                sentences.append(tail)\n",
    "            return sentences\n",
    "\n",
    "        # On coupe si derri√®re on a (√©ventuellement) guillemets/parenth√®ses puis espace\n",
    "        j = i + 1\n",
    "        while j < n and text[j] in ['\"', \"¬ª\", \"‚Äô\", \"'\", \")\", \"]\"]:\n",
    "            j += 1\n",
    "\n",
    "        if j < n and text[j].isspace():\n",
    "            k = j\n",
    "            while k < n and text[k].isspace():\n",
    "                k += 1\n",
    "\n",
    "            # D√©but plausible de phrase\n",
    "            if k < n and re.match(r\"[A-Z√Ä-√ñ√ò-√ù0-9¬´(\\[]\", text[k]):\n",
    "                sent = text[start:k].strip()\n",
    "                if sent:\n",
    "                    sentences.append(sent)\n",
    "                start = k\n",
    "\n",
    "    tail = text[start:].strip()\n",
    "    if tail:\n",
    "        sentences.append(tail)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def split_long_text_phrase_aware(text: str, max_chars: int, overlap_sentences: int = 1) -> List[str]:\n",
    "    \"\"\"\n",
    "    D√©coupe un texte long en segments <= max_chars en privil√©giant les fins de phrase.\n",
    "\n",
    "    overlap_sentences conserve un l√©ger chevauchement de phrases pour pr√©server le contexte.\n",
    "    \"\"\"\n",
    "    text = normalize_text(text)\n",
    "    if len(text) <= max_chars:\n",
    "        return [text]\n",
    "\n",
    "    sents = split_into_sentences(text)\n",
    "\n",
    "    # Fallback si segmentation en phrases impossible : d√©coupe en caract√®res avec overlap\n",
    "    if len(sents) <= 1:\n",
    "        parts: List[str] = []\n",
    "        overlap_chars = 120\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = min(len(text), start + max_chars)\n",
    "            parts.append(text[start:end].strip())\n",
    "            if end >= len(text):\n",
    "                break\n",
    "            start = max(0, end - overlap_chars)\n",
    "            if start >= end:\n",
    "                start = end\n",
    "        return [p for p in parts if len(p) >= 80]\n",
    "\n",
    "    segments: List[str] = []\n",
    "    cur: List[str] = []\n",
    "    cur_len = 0\n",
    "\n",
    "    for s in sents:\n",
    "        s = normalize_text(s)\n",
    "        if not s:\n",
    "            continue\n",
    "\n",
    "        # Cas rare : une phrase seule d√©passe max_chars -> fallback caract√®re\n",
    "        if len(s) > max_chars:\n",
    "            if cur:\n",
    "                segments.append(normalize_text(\" \".join(cur)))\n",
    "                cur, cur_len = [], 0\n",
    "            segments.extend(split_long_text_phrase_aware(s, max_chars=max_chars, overlap_sentences=0))\n",
    "            continue\n",
    "\n",
    "        add_len = len(s) + (1 if cur_len else 0)\n",
    "        if cur and (cur_len + add_len > max_chars):\n",
    "            segments.append(normalize_text(\" \".join(cur)))\n",
    "\n",
    "            if overlap_sentences > 0:\n",
    "                cur = cur[-overlap_sentences:]\n",
    "                cur_len = sum(len(x) for x in cur) + max(0, len(cur) - 1)\n",
    "            else:\n",
    "                cur, cur_len = [], 0\n",
    "\n",
    "        cur.append(s)\n",
    "        cur_len += add_len\n",
    "\n",
    "    if cur:\n",
    "        segments.append(normalize_text(\" \".join(cur)))\n",
    "\n",
    "    return [seg for seg in segments if len(seg) >= 80]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# M√©tadonn√©es & liens\n",
    "# ----------------------------\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ArticleMeta:\n",
    "    \"\"\"M√©tadonn√©es minimales d'un article/document XML.\"\"\"\n",
    "    article_id: Optional[str]\n",
    "    num: Optional[str]\n",
    "    url: Optional[str]\n",
    "    etat: Optional[str]\n",
    "    date_debut: Optional[str]\n",
    "    date_fin: Optional[str]\n",
    "    titre: Optional[str]\n",
    "\n",
    "\n",
    "def extract_article_meta(root: ET.Element) -> ArticleMeta:\n",
    "    \"\"\"\n",
    "    Extrait des m√©tadonn√©es fr√©quentes dans les XML juridiques.\n",
    "\n",
    "    Notes :\n",
    "    - Les chemins sont bas√©s sur des structures typiques ; extraction tol√©rante\n",
    "      (beaucoup de fichiers ne contiennent pas tous les champs).\n",
    "    \"\"\"\n",
    "    article_id = (\n",
    "        safe_findtext(root, \".//META/META_COMMUN/ID\")\n",
    "        or safe_findtext(root, \".//ID\")\n",
    "    )\n",
    "    num = safe_findtext(root, \".//META/META_SPEC/META_ARTICLE/NUM\") or safe_findtext(root, \".//NUM\")\n",
    "    url = safe_findtext(root, \".//META/META_COMMUN/URL\") or safe_findtext(root, \".//URL\")\n",
    "    etat = safe_findtext(root, \".//META/META_SPEC/META_ARTICLE/ETAT\") or safe_findtext(root, \".//ETAT\")\n",
    "    date_debut = safe_findtext(root, \".//META/META_SPEC/META_ARTICLE/DATE_DEBUT\") or safe_findtext(root, \".//DATE_DEBUT\")\n",
    "    date_fin = safe_findtext(root, \".//META/META_SPEC/META_ARTICLE/DATE_FIN\") or safe_findtext(root, \".//DATE_FIN\")\n",
    "\n",
    "    # Tentative de titre (selon les structures disponibles)\n",
    "    titre = (\n",
    "        safe_findtext(root, \".//CONTEXTE//TITRE_TXT\")\n",
    "        or safe_findtext(root, \".//TITRE_TXT\")\n",
    "        or safe_findtext(root, \".//CONTEXTE//TITRE_TM\")\n",
    "        or safe_findtext(root, \".//TITRE_TM\")\n",
    "    )\n",
    "\n",
    "    return ArticleMeta(\n",
    "        article_id=article_id,\n",
    "        num=num,\n",
    "        url=url,\n",
    "        etat=etat,\n",
    "        date_debut=date_debut,\n",
    "        date_fin=date_fin,\n",
    "        titre=titre,\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_links(root: ET.Element, max_links: int = 200) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Extrait les liens juridiques (citations, versions, renvois) en m√©tadonn√©es.\n",
    "\n",
    "    On les conserve hors du texte index√© :\n",
    "    - utiles pour naviguer / expliquer / auditer\n",
    "    - mais bruit tr√®s fort pour le retrieval\n",
    "\n",
    "    max_links : limite de s√©curit√© pour √©viter d'embarquer des milliers de liens.\n",
    "    \"\"\"\n",
    "    links: List[Dict[str, str]] = []\n",
    "\n",
    "    for lien in root.findall(\".//LIEN\"):\n",
    "        payload: Dict[str, str] = {}\n",
    "        payload.update({k: v for k, v in lien.attrib.items() if v})\n",
    "        if lien.text:\n",
    "            payload[\"label\"] = normalize_text(lien.text)\n",
    "        if payload:\n",
    "            links.append(payload)\n",
    "        if len(links) >= max_links:\n",
    "            break\n",
    "\n",
    "    return links\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Extraction paragraphes\n",
    "# ----------------------------\n",
    "\n",
    "def _walk_collect_paragraphs(root: ET.Element, excluded_tags: Optional[set] = None) -> List[str]:\n",
    "    \"\"\"\n",
    "    Collecte r√©cursivement le texte des balises <p>, en excluant certaines sous-arborescences.\n",
    "\n",
    "    ElementTree \"standard\" ne fournit pas directement l'acc√®s au parent d'un n≈ìud.\n",
    "    Pour √©viter de r√©cup√©rer des paragraphes situ√©s dans des zones de liens ou d'historique,\n",
    "    on parcourt r√©cursivement l'arbre en maintenant un √©tat \"dans une zone exclue\".\n",
    "\n",
    "    excluded_tags : ensemble de noms de balises dont toute la sous-arborescence est ignor√©e\n",
    "                    (ex: LIENS, VERSIONS, META, CONTEXTE).\n",
    "    \"\"\"\n",
    "    if excluded_tags is None:\n",
    "        excluded_tags = {\"LIENS\", \"VERSIONS\"}\n",
    "\n",
    "    paragraphs: List[str] = []\n",
    "\n",
    "    def _dfs(node: ET.Element, in_excluded: bool) -> None:\n",
    "        now_excluded = in_excluded or (node.tag in excluded_tags)\n",
    "\n",
    "        if (not now_excluded) and node.tag == \"p\":\n",
    "            txt = \" \".join(iter_text_nodes(node))\n",
    "            txt = normalize_text(txt)\n",
    "            if txt:\n",
    "                paragraphs.append(txt)\n",
    "\n",
    "        for child in list(node):\n",
    "            _dfs(child, now_excluded)\n",
    "\n",
    "    _dfs(root, False)\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "def extract_paragraphs(root: ET.Element) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extrait une liste de paragraphes \"utiles\" en ignorant les zones bruyantes.\n",
    "\n",
    "    Strat√©gie (du plus fiable au plus permissif) :\n",
    "    1) Priorit√© √† BLOC_TEXTUEL/CONTENU//p.\n",
    "    2) Autres zones de contenu (ex: NOTA/CONTENU//p) si pr√©sentes.\n",
    "    3) Parcours r√©cursif : tous les <p> en excluant LIENS/VERSIONS/META/CONTEXTE.\n",
    "    4) Fallback ultime : texte global (sans LIEN).\n",
    "    \"\"\"\n",
    "    paragraphs: List[str] = []\n",
    "\n",
    "    # 1) Zone textuelle \"canonique\"\n",
    "    for p in root.findall(\".//BLOC_TEXTUEL//CONTENU//p\"):\n",
    "        txt = \" \".join(iter_text_nodes(p))\n",
    "        txt = normalize_text(txt)\n",
    "        if txt:\n",
    "            paragraphs.append(txt)\n",
    "\n",
    "    if paragraphs:\n",
    "        return paragraphs\n",
    "\n",
    "    # 2) Contenu parfois pr√©sent dans NOTA/CONTENU\n",
    "    for p in root.findall(\".//NOTA//CONTENU//p\"):\n",
    "        txt = \" \".join(iter_text_nodes(p))\n",
    "        txt = normalize_text(txt)\n",
    "        if txt:\n",
    "            paragraphs.append(txt)\n",
    "\n",
    "    if paragraphs:\n",
    "        return paragraphs\n",
    "\n",
    "    # 3) Parcours global en excluant les zones tr√®s bruyantes\n",
    "    paragraphs = _walk_collect_paragraphs(root, excluded_tags={\"LIENS\", \"VERSIONS\", \"META\", \"CONTEXTE\"})\n",
    "    if paragraphs:\n",
    "        return paragraphs\n",
    "\n",
    "    # 4) Fallback ultime : texte global en √©vitant la balise LIEN\n",
    "    texts: List[str] = []\n",
    "    for elem in root.iter():\n",
    "        if elem.tag == \"LIEN\":\n",
    "            continue\n",
    "        if elem.text:\n",
    "            t = normalize_text(elem.text)\n",
    "            if t:\n",
    "                texts.append(t)\n",
    "\n",
    "    blob = normalize_text(\" \".join(texts))\n",
    "    return [blob] if blob else []\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Chunking passage-level\n",
    "# ----------------------------\n",
    "\n",
    "def chunk_paragraphs(\n",
    "    paragraphs: List[str],\n",
    "    max_chars: int = 1200,\n",
    "    overlap_paragraphs: int = 1,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Regroupe des paragraphes en chunks \"passage-level\".\n",
    "\n",
    "    - On conserve au maximum des fronti√®res naturelles (paragraphes).\n",
    "    - Si un paragraphe d√©passe max_chars, on le d√©coupe en segments \"phrase-aware\"\n",
    "      pour √©viter de couper au milieu d'une phrase.\n",
    "    - Overlap l√©ger entre chunks pour pr√©server le contexte.\n",
    "    \"\"\"\n",
    "    if max_chars < 200:\n",
    "        raise ValueError(\"max_chars doit √™tre >= 200 pour rester utile.\")\n",
    "\n",
    "    flat_units: List[str] = []\n",
    "    for para in paragraphs:\n",
    "        para = normalize_text(para)\n",
    "        if not para:\n",
    "            continue\n",
    "\n",
    "        if len(para) <= max_chars:\n",
    "            flat_units.append(para)\n",
    "        else:\n",
    "            flat_units.extend(split_long_text_phrase_aware(para, max_chars=max_chars, overlap_sentences=1))\n",
    "\n",
    "    chunks: List[str] = []\n",
    "    cur: List[str] = []\n",
    "    cur_len = 0\n",
    "\n",
    "    for unit in flat_units:\n",
    "        unit = normalize_text(unit)\n",
    "        if not unit:\n",
    "            continue\n",
    "\n",
    "        add_len = len(unit) + (1 if cur_len else 0)\n",
    "        if cur and (cur_len + add_len > max_chars):\n",
    "            chunks.append(normalize_text(\" \".join(cur)))\n",
    "\n",
    "            if overlap_paragraphs > 0:\n",
    "                cur = cur[-overlap_paragraphs:]\n",
    "                cur_len = sum(len(x) for x in cur) + max(0, len(cur) - 1)\n",
    "            else:\n",
    "                cur, cur_len = [], 0\n",
    "\n",
    "        cur.append(unit)\n",
    "        cur_len += add_len\n",
    "\n",
    "    if cur:\n",
    "        chunks.append(normalize_text(\" \".join(cur)))\n",
    "\n",
    "    return [c for c in chunks if len(c) >= 80]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def stable_chunk_id(doc_id: str, chunk_index: int, text: str) -> str:\n",
    "    \"\"\"\n",
    "    G√©n√®re un identifiant stable et compact pour un chunk.\n",
    "    Le hash d√©pend :\n",
    "    - doc_id (chemin)\n",
    "    - index du chunk\n",
    "    - texte du chunk\n",
    "    \"\"\"\n",
    "    h = hashlib.sha1()\n",
    "    h.update(doc_id.encode(\"utf-8\", errors=\"ignore\"))\n",
    "    h.update(f\"::{chunk_index}::\".encode(\"utf-8\"))\n",
    "    h.update(text.encode(\"utf-8\", errors=\"ignore\"))\n",
    "    return h.hexdigest()[:16]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Parcours corpus\n",
    "# ----------------------------\n",
    "\n",
    "def iter_xml_files(data_root: str) -> Iterable[str]:\n",
    "    \"\"\"It√®re r√©cursivement sur tous les .xml d'un r√©pertoire.\"\"\"\n",
    "    for root_dir, _, files in os.walk(data_root):\n",
    "        for fn in files:\n",
    "            if fn.lower().endswith(\".xml\"):\n",
    "                yield os.path.join(root_dir, fn)\n",
    "\n",
    "\n",
    "def build_chunk_corpus_for_file(\n",
    "    xml_path: str,\n",
    "    max_chars: int = 1200,\n",
    "    overlap_paragraphs: int = 1,\n",
    "    keep_links: bool = True,\n",
    "    links_sample_size: int = 20,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Parse un fichier XML et retourne une liste de chunks indexables.\n",
    "\n",
    "    Chaque chunk inclut :\n",
    "    - doc_id / doc_type / chunk_index / chunk_id\n",
    "    - text (nettoy√©)\n",
    "    - meta (m√©tadonn√©es minimales)\n",
    "    - links_count + links_sample (audit, non index√©)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Rend la recherche de balises robuste m√™me si le XML utilise des namespaces\n",
    "        strip_namespaces(root)\n",
    "\n",
    "        meta = extract_article_meta(root)\n",
    "        paragraphs = extract_paragraphs(root)\n",
    "        doc_type = infer_doc_type_from_path(xml_path)\n",
    "\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "    if not paragraphs:\n",
    "        return []\n",
    "\n",
    "    chunks_text = chunk_paragraphs(\n",
    "        paragraphs=paragraphs,\n",
    "        max_chars=max_chars,\n",
    "        overlap_paragraphs=overlap_paragraphs,\n",
    "    )\n",
    "    if not chunks_text:\n",
    "        return []\n",
    "\n",
    "    links_all = extract_links(root) if keep_links else []\n",
    "    links_count = len(links_all)\n",
    "    links_sample = links_all[:max(0, links_sample_size)] if keep_links else []\n",
    "\n",
    "    out: List[Dict] = []\n",
    "    for i, chunk_text in enumerate(chunks_text):\n",
    "        # Nettoyage conservateur : supprime certains en-t√™tes techniques observ√©s\n",
    "        cleaned = strip_technical_header(chunk_text)\n",
    "\n",
    "        # S√©curit√© : si le nettoyage vide trop le texte, on garde l'original\n",
    "        if not cleaned or len(cleaned) < 50:\n",
    "            cleaned = chunk_text\n",
    "\n",
    "        out.append({\n",
    "            \"doc_id\": xml_path,\n",
    "            \"doc_type\": doc_type,\n",
    "            \"chunk_index\": i,\n",
    "\n",
    "            # L'identifiant refl√®te exactement le texte r√©ellement index√©\n",
    "            \"chunk_id\": stable_chunk_id(xml_path, i, cleaned),\n",
    "\n",
    "            # Texte r√©ellement index√©\n",
    "            \"text\": cleaned,\n",
    "\n",
    "            \"meta\": {\n",
    "                \"article_id\": meta.article_id,\n",
    "                \"num\": meta.num,\n",
    "                \"url\": meta.url,\n",
    "                \"etat\": meta.etat,\n",
    "                \"date_debut\": meta.date_debut,\n",
    "                \"date_fin\": meta.date_fin,\n",
    "                \"titre\": meta.titre,\n",
    "            },\n",
    "            \"links_count\": links_count,\n",
    "            \"links_sample\": links_sample,  # √©chantillon pour audit (non index√©)\n",
    "        })\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def build_chunk_corpus(\n",
    "    data_root: str,\n",
    "    max_chars: int = 1200,\n",
    "    overlap_paragraphs: int = 1,\n",
    "    min_text_len: int = 200,\n",
    "    keep_links: bool = True,\n",
    "    links_sample_size: int = 20,\n",
    "    limit_files: Optional[int] = None,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Construit un corpus chunk√© √† partir d'un r√©pertoire XML.\n",
    "\n",
    "    min_text_len : filtre \"anti-documents pauvres\" (appliqu√© sur la concat des chunks).\n",
    "    limit_files  : utile pour un test rapide.\n",
    "    \"\"\"\n",
    "    corpus: List[Dict] = []\n",
    "    seen_files = 0\n",
    "\n",
    "    for xml_path in iter_xml_files(data_root):\n",
    "        chunks = build_chunk_corpus_for_file(\n",
    "            xml_path=xml_path,\n",
    "            max_chars=max_chars,\n",
    "            overlap_paragraphs=overlap_paragraphs,\n",
    "            keep_links=keep_links,\n",
    "            links_sample_size=links_sample_size,\n",
    "        )\n",
    "\n",
    "        if chunks:\n",
    "            total_len = sum(len(c[\"text\"]) for c in chunks)\n",
    "            if total_len >= min_text_len:\n",
    "                corpus.extend(chunks)\n",
    "\n",
    "        seen_files += 1\n",
    "        if limit_files is not None and seen_files >= limit_files:\n",
    "            break\n",
    "\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def save_jsonl(records: List[Dict], out_path: str) -> None:\n",
    "    \"\"\"Sauvegarde une liste de dicts au format JSONL (1 dict par ligne).\"\"\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in records:\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "def print_quick_stats(records: List[Dict]) -> None:\n",
    "    \"\"\"Affiche quelques stats rapides pour v√©rifier l'effet du chunking.\"\"\"\n",
    "    if not records:\n",
    "        print(\"Aucun chunk g√©n√©r√©.\")\n",
    "        return\n",
    "\n",
    "    texts = [r[\"text\"] for r in records]\n",
    "    lengths = [len(t) for t in texts]\n",
    "    n_chunks = len(records)\n",
    "    n_docs = len({r[\"doc_id\"] for r in records})\n",
    "\n",
    "    avg_len = sum(lengths) / n_chunks\n",
    "    min_len = min(lengths)\n",
    "    max_len = max(lengths)\n",
    "\n",
    "    # Information de contexte : volume de liens (m√©tadonn√©es) associ√© aux documents\n",
    "    links_counts = [int(r.get(\"links_count\", 0)) for r in records]\n",
    "    avg_links = sum(links_counts) / n_chunks\n",
    "\n",
    "    print(f\"Documents sources : {n_docs}\")\n",
    "    print(f\"Chunks g√©n√©r√©s     : {n_chunks}\")\n",
    "    print(f\"Taille chunk (chars) ‚Äî min/avg/max : {min_len} / {avg_len:.1f} / {max_len}\")\n",
    "    print(f\"Liens (m√©tadonn√©es) ‚Äî moyenne par chunk : {avg_links:.1f}\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# CLI + mode \"IDE (Spyder)\"\n",
    "# ----------------------------\n",
    "\n",
    "def default_run_config() -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    D√©finit la configuration par d√©faut utilis√©e quand le script est lanc√© sans arguments\n",
    "    (ex: bouton \"Run\" de Spyder).\n",
    "\n",
    "    Remplace simplement les chemins et param√®tres ci-dessous selon notre machine.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        # Chemin racine de notre corpus XML\n",
    "        \"data_root\": r\"D:\\-- Projet RAG Avocats --\\data_main\\data\",\n",
    "\n",
    "        # Fichier de sortie JSONL\n",
    "        \"out_jsonl\": r\"D:\\-- Projet RAG Avocats --\\data_main\\result_tests\\corpus_chunks.jsonl\",\n",
    "\n",
    "        # Param√®tres de chunking\n",
    "        \"max_chars\": 1200,\n",
    "        \"overlap_paragraphs\": 1,\n",
    "\n",
    "        # Filtre anti-documents pauvres (somme des longueurs des chunks par doc)\n",
    "        \"min_text_len\": 200,\n",
    "\n",
    "        # Liens en m√©tadonn√©es (audit) : True garde, False ignore\n",
    "        \"keep_links\": True,\n",
    "        \"links_sample_size\": 20,\n",
    "\n",
    "        # Optionnel : limiter le nombre de fichiers pour un test rapide\n",
    "        \"limit_files\": None,  # ex: 200\n",
    "    }\n",
    "\n",
    "\n",
    "def parse_args(argv: Optional[List[str]] = None) -> argparse.Namespace:\n",
    "    \"\"\"Parse les arguments CLI.\"\"\"\n",
    "    p = argparse.ArgumentParser(description=\"Chunker un corpus XML juridique en passages propres (JSONL).\")\n",
    "    p.add_argument(\"--data-root\", required=True, help=\"R√©pertoire racine contenant les XML.\")\n",
    "    p.add_argument(\"--out-jsonl\", required=True, help=\"Chemin de sortie JSONL.\")\n",
    "    p.add_argument(\"--max-chars\", type=int, default=1200, help=\"Taille max (caract√®res) d'un chunk.\")\n",
    "    p.add_argument(\"--overlap-paragraphs\", type=int, default=1, help=\"Nombre de paragraphes de chevauchement.\")\n",
    "    p.add_argument(\"--min-text-len\", type=int, default=200, help=\"Filtre : longueur minimale totale de texte par document.\")\n",
    "    p.add_argument(\"--no-links\", action=\"store_true\", help=\"Ne pas extraire les liens en m√©tadonn√©es.\")\n",
    "    p.add_argument(\"--links-sample-size\", type=int, default=20, help=\"Taille de l'√©chantillon de liens conserv√© par chunk (audit).\")\n",
    "    p.add_argument(\"--limit-files\", type=int, default=None, help=\"Limiter le nombre de fichiers (test rapide).\")\n",
    "    return p.parse_args(argv)\n",
    "\n",
    "\n",
    "def run_with_config(cfg: Dict[str, object]) -> int:\n",
    "    \"\"\"\n",
    "    Ex√©cute la construction du corpus chunk√© √† partir d'un dictionnaire de configuration.\n",
    "\n",
    "    Ce wrapper √©vite de dupliquer la logique entre le mode CLI et le mode IDE.\n",
    "    \"\"\"\n",
    "    records = build_chunk_corpus(\n",
    "        data_root=str(cfg[\"data_root\"]),\n",
    "        max_chars=int(cfg[\"max_chars\"]),\n",
    "        overlap_paragraphs=int(cfg[\"overlap_paragraphs\"]),\n",
    "        min_text_len=int(cfg[\"min_text_len\"]),\n",
    "        keep_links=bool(cfg[\"keep_links\"]),\n",
    "        links_sample_size=int(cfg[\"links_sample_size\"]),\n",
    "        limit_files=cfg[\"limit_files\"],\n",
    "    )\n",
    "\n",
    "    out_path = str(cfg[\"out_jsonl\"])\n",
    "    save_jsonl(records, out_path)\n",
    "\n",
    "    print(f\"Corpus chunk√© √©crit : {out_path}\")\n",
    "    print_quick_stats(records)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def main(argv: Optional[List[str]] = None) -> int:\n",
    "    \"\"\"\n",
    "    Point d'entr√©e du script.\n",
    "\n",
    "    - Si on fournit des arguments (mode terminal), on parse la CLI.\n",
    "    - Si aucun argument n'est fourni (Spyder / Run), on utilise la config par d√©faut.\n",
    "    \"\"\"\n",
    "    # Cas typique Spyder : sys.argv contient uniquement le nom du script\n",
    "    no_cli_args = (argv is None and len(sys.argv) <= 1)\n",
    "\n",
    "    if no_cli_args:\n",
    "        cfg = default_run_config()\n",
    "\n",
    "        # S√©curit√© minimale : √©viter de lancer un run sur un chemin vide par erreur\n",
    "        if not cfg[\"data_root\"] or not cfg[\"out_jsonl\"]:\n",
    "            raise ValueError(\"Configurer 'data_root' et 'out_jsonl' dans default_run_config().\")\n",
    "\n",
    "        return run_with_config(cfg)\n",
    "\n",
    "    # Mode CLI explicite (terminal)\n",
    "    args = parse_args(argv)\n",
    "    cfg = {\n",
    "        \"data_root\": args.data_root,\n",
    "        \"out_jsonl\": args.out_jsonl,\n",
    "        \"max_chars\": args.max_chars,\n",
    "        \"overlap_paragraphs\": args.overlap_paragraphs,\n",
    "        \"min_text_len\": args.min_text_len,\n",
    "        \"keep_links\": (not args.no_links),\n",
    "        \"links_sample_size\": args.links_sample_size,\n",
    "        \"limit_files\": args.limit_files,\n",
    "    }\n",
    "    return run_with_config(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d419cf4",
   "metadata": {},
   "source": [
    "## ‚ñ∂Ô∏è Ex√©cuter (exemple)\n",
    "1) Ajuste `data_root` et `out_jsonl` selon notre machine.\n",
    "2) Lance `run_with_config(cfg)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6effe08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus chunk√© √©crit : D:\\-- Projet RAG Avocats --\\data_main\\result_tests\\corpus_chunks.jsonl\n",
      "Documents sources : 4091\n",
      "Chunks g√©n√©r√©s     : 13180\n",
      "Taille chunk (chars) ‚Äî min/avg/max : 86 / 942.8 / 3002\n",
      "Liens (m√©tadonn√©es) ‚Äî moyenne par chunk : 26.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemple notebook : ex√©cuter avec la configuration par d√©faut, puis ajuster les chemins.\n",
    "cfg = default_run_config()\n",
    "\n",
    "# TODO: Adapter ces 2 chemins pour notre environnement\n",
    "# cfg[\"data_root\"] = r\"D:\\...\\data\"\n",
    "# cfg[\"out_jsonl\"] = r\"D:\\...\\corpus_chunks.jsonl\"\n",
    "\n",
    "# Optionnel : test rapide\n",
    "# cfg[\"limit_files\"] = 200\n",
    "\n",
    "run_with_config(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646b3770",
   "metadata": {},
   "source": [
    "## Analyses\n",
    "\n",
    "### R√©sultats du run (corpus complet)\n",
    "- **Documents sources** : 4091  \n",
    "- **Chunks g√©n√©r√©s** : 13180  \n",
    "- **Taille des chunks (chars)** : min 86 / **moy 942.8** / max 3002  \n",
    "- **Liens d√©tect√©s (m√©tadonn√©es)** : **~26.8 liens/chunk** en moyenne\n",
    "\n",
    "### V√©rifications qualitatives (inspection guid√©e)\n",
    "Nous avons inspect√© 5 cas ‚Äúinformatifs‚Äù :\n",
    "1) **Chunk le plus long** : texte juridique exploitable (bon signal).\n",
    "2) **Chunk avec le plus de liens** : apr√®s nettoyage, suppression de l‚Äô**en-t√™te technique** (ID/URL/labels concat√©n√©s) ; le chunk d√©marre sur du contenu exploitable.\n",
    "3) **Document produisant le plus de chunks** : contenus longs (annexes/fiches) correctement segment√©s.\n",
    "4) **Chunk contenant un mot-cl√© (‚Äúcontrat‚Äù)** : contenu normatif bien r√©cup√©r√©.\n",
    "5) **Chunk le plus court** : normal (alin√©as courts / articles abrog√©s). √Ä filtrer √©ventuellement selon impact sur les m√©triques.\n",
    "\n",
    "### Points ‚ÄúOK‚Äù pour le retrieval et le RAG\n",
    "- Chunking **passage-level** : meilleur alignement question ‚Üî passage.\n",
    "- Champs de tra√ßabilit√© (`doc_id`, `chunk_index`, `chunk_id`) : permet de r√©cup√©rer les chunks voisins (i-1, i, i+1) et de reconstituer le contexte lors d‚Äôune r√©ponse LLM.\n",
    "- `doc_type` (article / section_ta / other) : permet de filtrer rapidement des cat√©gories potentiellement bruit√©es (ex: tables des mati√®res).\n",
    "\n",
    "### Pistes d‚Äôam√©lioration rapides (optionnelles)\n",
    "1) **Filtrage des chunks tr√®s courts** (ex: < 120‚Äì150 chars) si cela d√©grade BM25 (√† valider via benchmark).\n",
    "2) **Hybride** : BM25 ‚Üí candidats (top N) puis rerank dense (top K) : souvent un gain net en juridique.\n",
    "3) **Filtrage par date/version** (en vigueur √† la date X) via `date_debut`/`date_fin`.\n",
    "\n",
    "### Point important ‚Äúmission‚Äù\n",
    "Ce POC est r√©alis√© sur un corpus type L√©gifrance (FR) pour valider la **m√©thode** (extraction, chunking, benchmark).  \n",
    "Pour un RAG juridique sur la **l√©gislation marocaine**, l‚Äôindustrialisation n√©cessitera une adaptation aux formats/sources marocains (structure XML/HTML/PDF, m√©tadonn√©es, typologies, langues) et au versionning juridique.\n",
    "\n",
    "### Roadmap (industrialisation)\n",
    "Une vraie mise en production d‚Äôun RAG juridique n√©cessite une **√©tude syst√©matique du corpus** :\n",
    "- inventaire des balises/structures (XML), typologies de documents, qualit√© des m√©tadonn√©es,\n",
    "- gestion des versions/dates (‚Äúen vigueur √† la date X‚Äù),\n",
    "- strat√©gie d‚Äôextraction/chunking robuste et maintenable dans le temps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML-Pro-venv)",
   "language": "python",
   "name": "ml-pro-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

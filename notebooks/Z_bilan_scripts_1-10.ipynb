{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ea47a7",
   "metadata": {},
   "source": [
    "<span style=\"color:#8B949E;\">\n",
    "<b>Note de lecture</b> â€” Notebook issu de tests itÃ©ratifs (â€œspeed-testsâ€).  \n",
    "Le corpus utilisÃ© ici est un <b>bilan de ces scripts de test</b> (non alignÃ© client) uniquement pour valider la mÃ©canique (retrieval + Ã©valuation) et dÃ©rouler la roadmap.  \n",
    "Pour le chemin complet, suivre lâ€™ordre 01 â†’ 10 et lire en prioritÃ© les sections Markdown.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7370879",
   "metadata": {},
   "source": [
    "## <span style=\"color:green; font-weight:700;\">Phase 0</span> â€” <span style=\"color:#4DA3FF; font-weight:700;\">RÃ©cupÃ©ration & prise en main du corpus XML (juridique)</span>\n",
    "\n",
    "- **Objectif** : rÃ©cupÃ©rer un dump XML (textes de lois / codes / JO / jurisprudence selon source) et le **structurer â€œprÃªt pour un RAGâ€** : inventaire, qualitÃ©, versioning, filtrage, puis extraction/chunking. Test fait sur 1 fichier de `https://echanges.dila.gouv.fr/OPENDATA/LEGI/`\n",
    "- **Constats sur les XML** : structure â€œricheâ€ avec beaucoup de **liens internes** (rÃ©fÃ©rences dâ€™articles/sections), du **texte normatif**, et des **mÃ©tadonnÃ©es** (id, num, Ã©tat, dates, titres, hiÃ©rarchie, etc.) â†’ si on indexe tout brut, on indexe du bruit.\n",
    "- **Actions rÃ©alisÃ©es (POC)** :\n",
    "  - sÃ©lection dâ€™un sous-corpus (ex : *codes en vigueur*) + scripts de test progressifs ;\n",
    "  - inspection manuelle de quelques XML pour repÃ©rer les balises/sections utiles vs bruit ;\n",
    "  - mise en place dâ€™un **chunking passage-level** (Script 8) : extraction du texte utile, nettoyage minimal, tracking (`chunk_index`, `doc_type`) et conservation des mÃ©tadonnÃ©es.\n",
    "- **Point clÃ© â€œroadmapâ€** : une vraie mise en production dâ€™un RAG juridique exige une **Ã©tude systÃ©matique du corpus** (inventaire des structures/balises, qualitÃ© des mÃ©tadonnÃ©es, versions/dates, typologies de documents, Ã©volutions dans le temps) afin de dÃ©finir une stratÃ©gie dâ€™extraction + chunking **robuste et maintenable**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd876fb",
   "metadata": {},
   "source": [
    "--- \n",
    "## ğŸ§ª <span style=\"color:green; font-weight:700;\">Script 1</span> â€” <span style=\"color:orange; font-weight:800;\">1_BM25_simple.py</span>\n",
    "\n",
    "### 1) Phrase dâ€™introduction\n",
    "> ğŸ¯ **Â« Jâ€™ai commencÃ© par un BM25 pur sur des documents juridiques XML afin dâ€™identifier les limites du lexical (faux nÃ©gatifs, mauvais ranking) avant dâ€™introduire toute couche sÃ©mantique, et de garder un retrieval 100% explicable. Â»**\n",
    "\n",
    "### 2) âš™ï¸ Fonctions\n",
    "- **`extract_text_from_xml(xml_path)`**  \n",
    "  Extrait tout le texte dâ€™un XML (parcours des nÅ“uds) pour constituer une baseline dâ€™indexation BM25, en assumant un prÃ©processing minimal et donc du bruit XML.\n",
    "  - **Concepts Ã  expliciter** : baseline, bruit XML, extraction robuste, contenu indexable.\n",
    "\n",
    "- **`tokenize(text)`**  \n",
    "  Tokenise de faÃ§on volontairement simple (split) pour observer le comportement BM25 sans â€œmasquerâ€ les limites via une NLP plus complexe.\n",
    "  - **Concepts Ã  expliciter** : lexical pur, observabilitÃ©/debug, pas de magie.\n",
    "\n",
    "### 3) ğŸ” Analyse de fin de script\n",
    "- **Ce que le script dÃ©montre** : on peut construire un retriever BM25 explicable sur corpus juridique avec une extraction minimale.\n",
    "- **Limites mises en Ã©vidence** : mismatch vocabulaire utilisateur â†” vocabulaire juridique â†’ faux nÃ©gatifs et ranking faible sur certaines requÃªtes.\n",
    "- **Pourquoi câ€™est utile** : sert de socle mesurable pour justifier lâ€™ajout progressif (benchmark + filtres + query understanding + dense + chunking + hybride) dans les scripts suivants.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1f1652",
   "metadata": {},
   "source": [
    "--- \n",
    "## ğŸ§ª <span style=\"color:green; font-weight:700;\">Script 2</span> â€” <span style=\"color:orange; font-weight:800;\">2_Benchmark_BM25.py</span>\n",
    "\n",
    "### 1) Phrase dâ€™introduction\n",
    "> ğŸ¯ **Â« Notre benchmark est volontairement dÃ©couplÃ© du corpus et du moteur BM25 : il consomme un corpus normalisÃ© + un retriever afin de comparer des stratÃ©gies de retrieval sans changer la logique dâ€™Ã©valuation, avec mÃ©triques + VERBATIM pour diagnostiquer faux positifs et faux nÃ©gatifs. Â»**\n",
    "\n",
    "### 2) âš™ï¸ Fonctions\n",
    "- **`extract_text_from_xml(xml_path)`**  \n",
    "  Extrait le texte dâ€™un fichier XML de faÃ§on robuste (approche volontairement simple) pour constituer la matiÃ¨re Ã  indexer dans un POC benchmarkable.\n",
    "  - **Concepts Ã  expliciter** : extraction â€œbaselineâ€, bruit XML, corpus normalisÃ© (au sens â€œtexte exploitableâ€).\n",
    "\n",
    "- **`tokenize(text)`**  \n",
    "  Tokenise simplement le texte pour alimenter BM25, afin de garder un comportement lexical observable (et donc facile Ã  expliquer en entretien).\n",
    "  - **Concepts Ã  expliciter** : tokenisation minimaliste, retrieval lexical, observabilitÃ©.\n",
    "\n",
    "- **`bm25_search(query, documents, bm25, top_k=10)`**  \n",
    "  ExÃ©cute une recherche BM25 (scores puis tri) et retourne le top-k documents classÃ©s, base de notre Ã©valuation.\n",
    "  - **Concepts Ã  expliciter** : BM25 (scoring), top-k (liste triÃ©e), ranking.\n",
    "\n",
    "- **`is_relevant(document_text, relevant_keywords)`**  \n",
    "  ImplÃ©mente notre **oracle V1** : un document est marquÃ© **PERTINENT** sâ€™il contient au moins un des mots-clÃ©s attendus pour la question.\n",
    "  - **Concepts Ã  expliciter** : *oracle/ground truth du benchmark*, â€œpertinentâ€ (critÃ¨re binaire), faux positif / faux nÃ©gatif.\n",
    "\n",
    "- **`recall_at_k(results, relevant_keywords, k)`**  \n",
    "  Calcule **Recall@k** (binaire) : 1 si au moins un pertinent est dans le top-k, sinon 0.\n",
    "  - **Concepts Ã  expliciter** : retrieval rÃ©ussi vs Ã©chec, dÃ©pendance Ã  k, signal â€œretrouve / ne retrouve pasâ€.\n",
    "\n",
    "- **`reciprocal_rank(results, relevant_keywords)`**  \n",
    "  Calcule le **MRR** : 1 / rang du premier pertinent (0 si aucun pertinent), donc mesure la rapiditÃ© Ã  trouver â€œle bonâ€.\n",
    "  - **Concepts Ã  expliciter** : rang, â€œpremier pertinentâ€, qualitÃ© de ranking.\n",
    "\n",
    "- **`ndcg_at_k(results, relevant_keywords, k)`**  \n",
    "  Calcule **nDCG@k** : mesure la qualitÃ© globale du classement (rÃ©compense les pertinents haut placÃ©s, pÃ©nalise les pertinents bas placÃ©s).\n",
    "  - **Concepts Ã  expliciter** : DCG/IDCG, pÃ©nalisation du mauvais ranking, vision â€œranking globalâ€.\n",
    "\n",
    "- **`evaluate_benchmark(questions, documents, bm25, top_k=10)`**  \n",
    "  Lance le benchmark sur toutes les questions et calcule les moyennes Recall@k / MRR / nDCG@k.\n",
    "  - **Concepts Ã  expliciter** : protocole reproductible, agrÃ©gation, comparaison avant/aprÃ¨s.\n",
    "\n",
    "- **`display_results_per_query(questions, documents, bm25, top_k=10)`**  \n",
    "  Affiche le **VERBATIM par question** (top-k + marquage PERTINENT) pour expliquer les mÃ©triques et analyser les erreurs concrÃ¨tes.\n",
    "  - **Concepts Ã  expliciter** : audit qualitatif, explicabilitÃ©, diagnostic (faux positifs / faux nÃ©gatifs).\n",
    "\n",
    "### 3) ğŸ” Analyse de fin de script\n",
    "- **Ce que ce script apporte (par rapport au script 1)** : une **boucle dâ€™Ã©valuation stable** (mÃªmes questions, mÃªmes mÃ©triques, mÃªme oracle V1) qui rend le retrieval **mesurable et comparable** (exactement ce que demande la mission : amÃ©lioration prouvÃ©e par mesures, pas au feeling).\n",
    "- **RÃ©sultat baseline (tel que constatÃ© dans le script)** : BM25 retrouve au moins un pertinent pour **2 questions sur 3** dans le top-10 (Recall@10 â‰ˆ 0,667), mais le **premier pertinent** nâ€™est pas toujours en tÃªte (MRR â‰ˆ 0,375) et le ranking global reste bruitÃ© (nDCG@10 â‰ˆ 0,459).\n",
    "- **Lecture â€œerreursâ€ grÃ¢ce au VERBATIM** :\n",
    "  - Les requÃªtes lexicalement discriminantes marchent bien (vocabulaire direct).\n",
    "  - Les requÃªtes implicites/sÃ©mantiques gÃ©nÃ¨rent des faux nÃ©gatifs critiques (mismatch vocabulaire utilisateur â†” vocabulaire juridique).\n",
    "  - Une partie du bruit vient du pÃ©rimÃ¨tre corpus (documents hors sujet juridique immÃ©diat), ce qui justifie ensuite filtrage, query understanding, dense, puis chunking.\n",
    "- **Conclusion opÃ©rationnelle** : ce script fixe notre **point zÃ©ro chiffrÃ©** et rend explicables les Ã©checs ; câ€™est le socle nÃ©cessaire pour dÃ©montrer des gains mesurÃ©s dans les scripts suivants.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1deac9",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ§ª <span style=\"color:green; font-weight:700;\">Script 3</span> â€” <span style=\"color:orange; font-weight:800;\">3_Benchmark_filtre-BM25.py</span>\n",
    "\n",
    "### 1) Phrase dâ€™introduction (extraite / resserrÃ©e)\n",
    "> ğŸ¯ **Â« AprÃ¨s la baseline BM25 brute, jâ€™ai ajoutÃ© un filtrage mÃ©tier minimal (par code) pour rÃ©duire le bruit avant toute couche sÃ©mantique, puis jâ€™ai rejouÃ© exactement le mÃªme benchmark (Recall@k, MRR, nDCG + VERBATIM) afin de quantifier objectivement lâ€™impact du filtrage. Â»**\n",
    "\n",
    "### 2) âš™ï¸ Fonctions\n",
    "- **`tokenize(text)`**  \n",
    "  Tokenisation volontairement simple pour observer les limites dâ€™un retrieval lexical pur (BM25) sans complexifier lâ€™analyse.\n",
    "  - **Concepts Ã  expliciter** : lexical, observabilitÃ©, baseline.\n",
    "\n",
    "- **`bm25_search(query, documents, bm25, top_k=10)`**  \n",
    "  Calcule les scores BM25 sur le corpus, trie par score dÃ©croissant et retourne le top-k (la liste classÃ©e utilisÃ©e par les mÃ©triques et le verbatim).\n",
    "  - **Concepts Ã  expliciter** : scoring, ranking, top-k, â€œmoteurâ€ BM25.\n",
    "\n",
    "- **`is_relevant(document_text, relevant_keywords)`**  \n",
    "  ImplÃ©mente lâ€™**oracle V1** : marque un document **PERTINENT** sâ€™il contient au moins un des mots-clÃ©s attendus pour la question.\n",
    "  - **Concepts Ã  expliciter** : oracle/ground truth du benchmark, pertinent (critÃ¨re binaire), faux positif / faux nÃ©gatif.\n",
    "\n",
    "- **`recall_at_k(results, relevant_keywords, k)`**  \n",
    "  Calcule **Recall@k** (binaire) : 1 si au moins un pertinent est dans le top-k, sinon 0.\n",
    "  - **Concepts Ã  expliciter** : â€œretrouvÃ© vs non retrouvÃ©â€, sensibilitÃ© Ã  k.\n",
    "\n",
    "- **`reciprocal_rank(results, relevant_keywords)`**  \n",
    "  Calcule **MRR** : 1 / rang du premier pertinent (0 si aucun pertinent) pour mesurer la vitesse dâ€™accÃ¨s au premier bon rÃ©sultat.\n",
    "  - **Concepts Ã  expliciter** : rang, â€œpremier pertinentâ€, qualitÃ© de ranking.\n",
    "\n",
    "- **`ndcg_at_k(results, relevant_keywords, k)`**  \n",
    "  Calcule **nDCG@k** : mesure la qualitÃ© globale du classement (rÃ©compense les pertinents haut placÃ©s, pÃ©nalise les pertinents bas placÃ©s).\n",
    "  - **Concepts Ã  expliciter** : ranking global, pÃ©nalisation du mauvais ordre.\n",
    "\n",
    "- **`evaluate_benchmark(questions, documents, bm25, top_k=10)`**  \n",
    "  ExÃ©cute le benchmark sur toutes les questions et agrÃ¨ge les mÃ©triques moyennes (Recall@k, MRR, nDCG@k) pour comparer des variantes Ã  protocole constant.\n",
    "  - **Concepts Ã  expliciter** : protocole reproductible, comparaison â€œtoutes choses Ã©galesâ€.\n",
    "\n",
    "- **`display_results_per_query(questions, documents, bm25, top_k=10)`**  \n",
    "  Produit le **VERBATIM par question** : top-k + marquage PERTINENT, pour expliquer les mÃ©triques et diagnostiquer prÃ©cisÃ©ment les erreurs.\n",
    "  - **Concepts Ã  expliciter** : audit qualitatif, explicabilitÃ©, diagnostic des faux positifs / faux nÃ©gatifs.\n",
    "\n",
    "### 3) ğŸ” Analyse de fin de script\n",
    "- **Changement testÃ©** : uniquement le **pÃ©rimÃ¨tre documentaire** (filtrage mÃ©tier en amont, via `corpus_loader.py`), sans changer ni les questions ni les mÃ©triques.\n",
    "- **HypothÃ¨se** : en rÃ©duisant le bruit (documents hors pÃ©rimÃ¨tre), on rÃ©duit les **faux positifs** et on amÃ©liore potentiellement le **ranking** (MRR/nDCG), tout en surveillant le risque de perdre du **recall** si le filtre est trop agressif.\n",
    "- **Apport clÃ©** : lâ€™expÃ©rience dÃ©montre une mÃ©thode â€œproduction-friendlyâ€ : *pÃ©rimÃ¨tre constant + benchmark constant* â†’ on peut comparer proprement BM25, dense et hybride ensuite, sans biais expÃ©rimental.\n",
    "- **Transition logique** : une fois le bruit rÃ©duit, on peut introduire le dense/embeddings avec une lecture plus nette des gains (ce que font les scripts suivants).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d123940",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ§ª <span style=\"color:green; font-weight:700;\">Script 4</span> â€” <span style=\"color:orange; font-weight:800;\">4_dense_retrieval_embedding.py</span>\n",
    "\n",
    "### 1) Phrase dâ€™introduction\n",
    "> ğŸ¯ **Â« Pour mesurer lâ€™apport rÃ©el du retrieval sÃ©mantique, jâ€™ai gardÃ© exactement le mÃªme corpus filtrÃ©, le mÃªme benchmark et les mÃªmes mÃ©triques que pour BM25, et jâ€™ai uniquement remplacÃ© le retriever par des embeddings : toute variation observÃ©e est donc attribuable au dense retrieval. Â»**\n",
    "\n",
    "### 2) âš™ï¸ Fonctions\n",
    "- **`is_relevant(document_text, relevant_keywords)`**  \n",
    "  ImplÃ©mente lâ€™**oracle V1** : un document est marquÃ© **PERTINENT** sâ€™il contient au moins un des mots-clÃ©s attendus pour la question.\n",
    "  - **Concepts Ã  expliciter** : oracle/ground truth du benchmark, â€œpertinentâ€ (binaire), faux positif / faux nÃ©gatif.\n",
    "\n",
    "- **`recall_at_k(results, relevant_keywords, k)`**  \n",
    "  Calcule **Recall@k** (binaire) : 1 si au moins un pertinent apparaÃ®t dans le top-k, sinon 0.\n",
    "  - **Concepts Ã  expliciter** : succÃ¨s retrieval vs Ã©chec, dÃ©pendance Ã  k.\n",
    "\n",
    "- **`reciprocal_rank(results, relevant_keywords)`**  \n",
    "  Calcule **MRR** : 1 / rang du premier pertinent (0 si aucun), donc mesure la qualitÃ© de ranking â€œau premier bon rÃ©sultatâ€.\n",
    "  - **Concepts Ã  expliciter** : rang, â€œpremier pertinentâ€, vitesse dâ€™accÃ¨s au bon extrait.\n",
    "\n",
    "- **`ndcg_at_k(results, relevant_keywords, k)`**  \n",
    "  Calcule **nDCG@k** : mesure la qualitÃ© globale du classement (rÃ©compense les pertinents en haut de liste).\n",
    "  - **Concepts Ã  expliciter** : ranking global, pÃ©nalisation des pertinents mal classÃ©s.\n",
    "\n",
    "- **`dense_search(query, documents, document_embeddings, embedding_model, top_k=10)`**  \n",
    "  RÃ©alise une recherche sÃ©mantique : encode la requÃªte en embedding, calcule la **similaritÃ© cosinus** avec les embeddings des documents, trie et retourne le top-k.\n",
    "  - **Concepts Ã  expliciter** : embeddings (vecteurs), encodeur, similaritÃ© cosinus, retrieval sÃ©mantique, top-k.\n",
    "\n",
    "- **`evaluate_dense_benchmark(questions, documents, document_embeddings, embedding_model, k=10)`**  \n",
    "  ExÃ©cute le benchmark dense sur toutes les questions et agrÃ¨ge les mÃ©triques (Recall@k, MRR, nDCG@k) pour comparer proprement avec BM25.\n",
    "  - **Concepts Ã  expliciter** : protocole constant, comparaison â€œun seul changementâ€, amÃ©lioration prouvÃ©e par mesures.\n",
    "\n",
    "- **`display_dense_results_per_query(questions, documents, document_embeddings, embedding_model, k=10)`**  \n",
    "  Affiche le **VERBATIM par question** : top-k + similaritÃ© + drapeau **PERTINENT/NON PERTINENT**, pour interprÃ©ter les mÃ©triques et analyser les gains/Ã©checs du sÃ©mantique.\n",
    "  - **Concepts Ã  expliciter** : audit qualitatif, explicabilitÃ©, diagnostic (mismatch lexical vs sÃ©mantique).\n",
    "\n",
    "### 3) ğŸ” Analyse de fin de script\n",
    "- **Changement testÃ©** : remplacement de BM25 par un **dense retriever** (embeddings + similaritÃ© cosinus), sans changer le corpus filtrÃ©, les questions, ni les mÃ©triques.\n",
    "- **Ce que ce script permet de dÃ©montrer en entretien** :\n",
    "  - on sait isoler lâ€™effet dâ€™un composant (ici : le retriever) â†’ approche **mesurable, stable, amÃ©liorable** ;\n",
    "  - le dense retrieval est particuliÃ¨rement pertinent quand la formulation utilisateur est **sÃ©mantique / paraphrasÃ©e** (synonymes, notions implicites) lÃ  oÃ¹ le lexical peut Ã©chouer.\n",
    "- **Ce que le VERBATIM permet de trancher** :\n",
    "  - si le gain vient dâ€™un meilleur **recall** (on retrouve enfin un pertinent),\n",
    "  - ou dâ€™un meilleur **ranking** (le pertinent existait dÃ©jÃ  mais remonte dans le top-k),\n",
    "  - ou au contraire si le dense introduit du **hors-sujet sÃ©mantiquement proche** (faux positifs).\n",
    "- **Transition logique** : si le dense amÃ©liore certaines requÃªtes mais reste bruitÃ©, la suite naturelle est (1) mieux structurer la requÃªte (query understanding), puis (2) amÃ©liorer la granularitÃ© du corpus (chunking) et enfin (3) hybrider BM25+dense (RRF) pour cumuler signaux lexical + sÃ©mantique.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff22a2f",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ§ª <span style=\"color:green; font-weight:700;\">Script 5</span> â€” <span style=\"color:orange; font-weight:800;\">5_bm25_with_query_understanding.py</span>\n",
    "\n",
    "### 1) Phrase dâ€™introduction\n",
    "> ğŸ¯ **Â« Pour coller Ã  lâ€™attendu â€œquestion utilisateur â†’ bons extraitsâ€, jâ€™ai introduit une brique dÃ©terministe de *query understanding* (dictionnaire mÃ©tier) afin dâ€™enrichir la requÃªte avant retrieval, puis jâ€™ai comparÃ© Ã  protocole constant BM25 â€œrequÃªte bruteâ€ vs BM25 â€œrequÃªte enrichieâ€ (mÃ©triques avant/aprÃ¨s). Â»**\n",
    "\n",
    "### 2) âš™ï¸ Fonctions\n",
    "- **`tokenize(text)`**  \n",
    "  Tokenise le texte en mots (regex) pour alimenter BM25 de faÃ§on stable et explicable.\n",
    "  - **Concepts Ã  expliciter** : retrieval lexical, tokens, observabilitÃ©.\n",
    "\n",
    "- **`bm25_search(query, k=10)`**  \n",
    "  Calcule les scores BM25 de la requÃªte sur tout le corpus, trie les couples (document, score), et retourne le top-k.\n",
    "  - **Concepts Ã  expliciter** : scoring, ranking, top-k, baseline explicable.\n",
    "\n",
    "- **`evaluate_bm25(use_query_understanding=False)`**  \n",
    "  ExÃ©cute le benchmark sur toutes les questions en optionnant lâ€™enrichissement de requÃªte :  \n",
    "  - si `False` â†’ requÃªte brute,  \n",
    "  - si `True` â†’ requÃªte enrichie via `process_user_query(...)` (dictionnaire mÃ©tier),  \n",
    "  puis calcule les moyennes **Recall@10 / MRR / nDCG@10**.\n",
    "  - **Concepts Ã  expliciter** :\n",
    "    - **Query understanding (dÃ©terministe)** : transformation contrÃ´lÃ©e de la requÃªte, audit-able, versionnable.\n",
    "    - **`enriched_query`** : texte final rÃ©ellement envoyÃ© au retriever (important pour prouver ce qui est testÃ©).\n",
    "    - **Oracle V1** (implicite via `relevant_keywords`) : rÃ¨gle qui dÃ©finit â€œPERTINENTâ€ pour mesurer Recall/MRR/nDCG.\n",
    "\n",
    "### 3) ğŸ” Analyse de fin de script\n",
    "- **But de lâ€™expÃ©rience** : vÃ©rifier si un enrichissement mÃ©tier â€œsans LLMâ€ amÃ©liore dÃ©jÃ  un retriever lexical (BM25), et surtout Ã©tablir une mÃ©thode â€œavant/aprÃ¨sâ€ mesurable.\n",
    "- **Lecture attendue des rÃ©sultats** :\n",
    "  - Si lâ€™enrichissement ajoute des termes juridiques absents de la requÃªte, BM25 peut gagner (meilleur recall ou meilleur rang du premier pertinent).\n",
    "  - Si la requÃªte brute contenait dÃ©jÃ  le vocabulaire principal, lâ€™effet sur BM25 peut Ãªtre faible (normal : BM25 reste lexical).\n",
    "- **Pourquoi câ€™est utile pour la mission** :\n",
    "  - on dÃ©montre une brique *stable et amÃ©liorable* (dictionnaire versionnable),\n",
    "  - on instrumente lâ€™impact avec des mÃ©triques (pas au feeling),\n",
    "  - et on prÃ©pare la suite : ce mÃªme enrichissement devient souvent plus payant sur **dense** et **hybride** (scripts suivants), oÃ¹ la reformulation sÃ©mantique compte davantage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adfd98e",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ§ª <span style=\"color:green; font-weight:700;\">Script 6</span> â€” <span style=\"color:orange; font-weight:800;\">6_bm25_filtered_with_query_understanding.py</span>\n",
    "\n",
    "### 1) Phrase dâ€™introduction (extraite / resserrÃ©e)\n",
    "> ğŸ¯ **Â« AprÃ¨s BM25 + query understanding, jâ€™ajoute un filtrage mÃ©tier explicite (â€œCode du travailâ€) sur le corpus avant indexation, puis je rejoue le mÃªme benchmark pour mesurer si la rÃ©duction de bruit + lâ€™enrichissement de requÃªte amÃ©liorent Recall@10 / MRR / nDCG@10. Â»**\n",
    "\n",
    "### 2) âš™ï¸ Fonctions\n",
    "- **`tokenize(text)`**  \n",
    "  DÃ©coupe le texte en tokens (mots) en minuscules via regex afin dâ€™alimenter BM25 de faÃ§on stable et reproductible.\n",
    "  - **Concepts Ã  expliciter** : tokenisation, lexical, normalisation (lowercase), reproductibilitÃ©.\n",
    "\n",
    "- **`bm25_search(query, k=10)`**  \n",
    "  Calcule les scores BM25 de la requÃªte sur les **documents filtrÃ©s** (`filtered_documents`), trie par score dÃ©croissant et retourne le top-k.\n",
    "  - **Concepts Ã  expliciter** : scoring BM25, ranking, top-k, pÃ©rimÃ¨tre filtrÃ©, rÃ©duction du bruit.\n",
    "\n",
    "- **`evaluate()`**  \n",
    "  ExÃ©cute le benchmark sur toutes les questions : enrichit chaque requÃªte via `process_user_query(..., dictionary)`, interroge BM25, puis agrÃ¨ge **Recall@10 / MRR / nDCG@10**.\n",
    "  - **Concepts Ã  expliciter** :\n",
    "    - **Query understanding** : enrichissement dÃ©terministe via dictionnaire (`enriched_query` = requÃªte rÃ©ellement envoyÃ©e au retriever).\n",
    "    - **Oracle V1** : la pertinence est Ã©valuÃ©e via `relevant_keywords` (mots-clÃ©s attendus).\n",
    "    - **Mesure â€œavant/aprÃ¨sâ€** : protocole constant, seul le filtre + enrichissement changent.\n",
    "\n",
    "### 2bis) ğŸ§° Variables / blocs importants\n",
    "- **`dictionary = load_juridical_dictionary(...)`** : charge le rÃ©fÃ©rentiel mÃ©tier (intentions utilisateur â†’ concepts juridiques â†’ cibles).\n",
    "- **`filtered_documents = [...] if \"code du travail\" in doc[\"text\"].lower()`** : filtrage pÃ©rimÃ¨tre â€œmÃ©tierâ€ (rÃ©duit le bruit inter-code).\n",
    "- **`bm25 = BM25Okapi(tokenized_docs)`** : index BM25 construit uniquement sur le pÃ©rimÃ¨tre filtrÃ©.\n",
    "\n",
    "### 3) ğŸ” Analyse de fin de script\n",
    "- **HypothÃ¨se testÃ©e** : en rÃ©duisant le bruit (filtrage â€œCode du travailâ€) *et* en enrichissant la requÃªte (dictionnaire mÃ©tier), on augmente :\n",
    "  - soit le **recall** (on retrouve enfin un pertinent dans le top-10),\n",
    "  - soit la **qualitÃ© de ranking** (le pertinent remonte â†’ MRR/nDCG).\n",
    "- **Ce que ce script dÃ©montre mÃ©thodologiquement** : une approche â€œproduction-friendlyâ€ oÃ¹ la performance dÃ©pend autant de la **qualitÃ© du pÃ©rimÃ¨tre (filtre)** et de la **normalisation de requÃªte** que du modÃ¨le.\n",
    "- **Limite assumÃ©e** : le filtre textuel `\"code du travail\"` reste une heuristique (utile en POC) ; lâ€™Ã©tape suivante logique est de filtrer via mÃ©tadonnÃ©es/identifiants plus robustes et/ou de passer au passage-level (chunking) pour mieux aligner question â†” extrait.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1783a9a",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ§ª <span style=\"color:green; font-weight:700;\">Script 7</span> â€” <span style=\"color:orange; font-weight:800;\">7_dense_with_query_understanding.py</span>\n",
    "\n",
    "### 1) Phrase dâ€™introduction (extraite / resserrÃ©e)\n",
    "> ğŸ¯ **Â« Jâ€™ai Ã©valuÃ© un retrieval sÃ©mantique (embeddings) combinÃ© Ã  une requÃªte enrichie par dictionnaire mÃ©tier, Ã  protocole constant (mÃªmes questions, mÃªmes mÃ©triques), pour mesurer lâ€™impact rÃ©el sur la chaÃ®ne â€œquestion utilisateur â†’ bons extraitsâ€. Â»**\n",
    "\n",
    "### 2) âš™ï¸ Fonctions\n",
    "- **`dense_search(query, k=10)`**  \n",
    "  Encode la requÃªte en embedding, calcule la similaritÃ© cosinus avec les embeddings des documents, trie par score dÃ©croissant et renvoie le top-k.\n",
    "  - **Concepts Ã  expliciter** :\n",
    "    - **Embeddings** : reprÃ©sentation vectorielle du texte (sÃ©mantique).\n",
    "    - **`normalize_embeddings=True`** : normalisation des vecteurs pour rendre la cosinus cohÃ©rente.\n",
    "    - **Cosine similarity** : score de proximitÃ© sÃ©mantique utilisÃ© pour classer les documents.\n",
    "    - **Top-k** : liste ordonnÃ©e des meilleurs candidats â€œbons extraitsâ€ (avant gÃ©nÃ©ration).\n",
    "\n",
    "- **`evaluate()`**  \n",
    "  Pour chaque question du benchmark, enrichit la requÃªte via `process_user_query(..., dictionary)` (on utilise `enriched_query`), lance `dense_search`, puis calcule et moyenne **Recall@10**, **MRR**, **nDCG@10** via lâ€™oracle V1 (`relevant_keywords`).\n",
    "  - **Concepts Ã  expliciter** :\n",
    "    - **Query understanding (dÃ©terministe)** : enrichissement versionnable/auditable basÃ© sur un dictionnaire mÃ©tier.\n",
    "    - **`enriched_query`** : requÃªte rÃ©ellement envoyÃ©e au retriever (ce qui est mesurÃ©).\n",
    "    - **Oracle V1** : rÃ¨gle de pertinence binaire â€œPERTINENTâ€ basÃ©e sur des mots-clÃ©s attendus (`relevant_keywords`).\n",
    "    - **Recall@10 / MRR / nDCG@10** : mesure du retrieval (retrouve / rang du 1er pertinent / qualitÃ© globale du ranking).\n",
    "\n",
    "### 2bis) ğŸ§° Blocs/variables importants\n",
    "- **`dictionary = load_juridical_dictionary(\"juridical_dictionary.yml\")`** : charge le rÃ©fÃ©rentiel mÃ©tier (intentions utilisateur â†’ concepts juridiques â†’ cibles).\n",
    "- **`model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")`** : encodeur dâ€™embeddings multilingue (FR OK).\n",
    "- **`doc_embeddings = model.encode(doc_texts, normalize_embeddings=True, show_progress_bar=True)`** : prÃ©-calcul des embeddings documents (performance + stabilitÃ©), pour Ã©viter de rÃ©-encoder Ã  chaque requÃªte.\n",
    "\n",
    "### 3) ğŸ” Analyse de fin de script\n",
    "- **Protocole strict** : mÃªmes questions, mÃªmes mÃ©triques, seul le retriever change â†’ les gains observÃ©s sont attribuables au dense retrieval + requÃªte enrichie.\n",
    "- **RÃ©sultats (tels que synthÃ©tisÃ©s en fin de script)** :\n",
    "  - BM25 (requÃªte brute / enrichie / filtrÃ©e+enrichie) : **Recall@10 = 0,33**, **MRR = 0,33**, **nDCG@10 = 0,33**.\n",
    "  - Dense + requÃªte enrichie (ce script) : **Recall@10 = 0,67**, **MRR â‰ˆ 0,38**, **nDCG@10 â‰ˆ 0,40**.\n",
    "- **InterprÃ©tation** :\n",
    "  - Le gain principal est un **meilleur recall** (on retrouve un pertinent sur 2 questions sur 3 au lieu de 1 sur 3).\n",
    "  - Le dense â€œamplifieâ€ une requÃªte mieux posÃ©e : la couche mÃ©tier (query understanding) sert Ã  rapprocher le langage utilisateur du langage juridique.\n",
    "- **Implication â€œproduction RAGâ€** :\n",
    "  - On peut prouver une amÃ©lioration sans â€œfeelingâ€ (mÃ©triques avant/aprÃ¨s).\n",
    "  - Le retrieval dense est une brique utile, mais doit rester pilotÃ©e par (1) comprÃ©hension mÃ©tier explicite et (2) audit/benchmark, avant dâ€™aller vers chunking et hybride.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955a36c1",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ§ª <span style=\"color:green; font-weight:700;\">Script 8</span> â€” <span style=\"color:orange; font-weight:800;\">8_corpus_chunker_xml.py</span>\n",
    "\n",
    "### 1) Phrase dâ€™introduction\n",
    "> ğŸ¯ **Â« Jâ€™ai transformÃ© les XML juridiques en un corpus â€œpassage-levelâ€ (JSONL) : des chunks propres, traÃ§ables et filtrables, afin dâ€™amÃ©liorer la prÃ©cision du retrieval et de rendre lâ€™Ã©valuation/audit (pertinence, citations, erreurs) rÃ©ellement exploitables pour un RAG. Â»**\n",
    "\n",
    "### 2) âš™ï¸ Fonctions\n",
    "- **`normalize_text(text)`**  \n",
    "  Normalise un texte (trim + espaces multiples) pour stabiliser lâ€™indexation et lâ€™affichage.\n",
    "  - **Concepts** : normalisation, stabilitÃ© du corpus.\n",
    "\n",
    "- **`strip_technical_header(text)`**  \n",
    "  Retire des en-tÃªtes techniques parasites (ID/URL/stats concatÃ©nÃ©s) en prÃ©fixe pour Ã©viter de polluer les chunks.\n",
    "  - **Concepts** : bruit technique, nettoyage conservateur.\n",
    "\n",
    "- **`safe_findtext(node, xpath, default=\"\")`**  \n",
    "  RÃ©cupÃ¨re un champ XML (findtext) de faÃ§on sÃ»re (valeur par dÃ©faut) pour Ã©viter les crashs sur XML incomplets.\n",
    "  - **Concepts** : robustesse, XML hÃ©tÃ©rogÃ¨ne.\n",
    "\n",
    "- **`iter_text_nodes(node)`**  \n",
    "  ItÃ¨re sur le texte utile des nÅ“uds XML (texte + tails) pour construire un contenu â€œlisibleâ€ Ã  partir de structures XML variÃ©es.\n",
    "  - **Concepts** : extraction robuste, texte â€œutileâ€.\n",
    "\n",
    "- **`strip_namespaces(xml_string)`**  \n",
    "  Supprime les namespaces XML pour simplifier les XPath et rendre lâ€™extraction plus stable.\n",
    "  - **Concepts** : namespaces, XPath, robustesse parsing.\n",
    "\n",
    "- **`infer_doc_type_from_path(xml_path)`**  \n",
    "  DÃ©duit un `doc_type` Ã  partir du chemin/fichier (ex: article, section, etc.) pour permettre des filtres mÃ©tier ensuite.\n",
    "  - **Concepts** : typologie documentaire, filtrage par mÃ©tadonnÃ©es.\n",
    "\n",
    "- **`_looks_like_abbrev_before_dot(s, idx)`**  \n",
    "  Heuristique pour dÃ©tecter des abrÃ©viations avant un point afin dâ€™Ã©viter de â€œcasserâ€ une phrase au mauvais endroit.\n",
    "  - **Concepts** : segmentation fiable, abrÃ©viations juridiques.\n",
    "\n",
    "- **`split_into_sentences(text)`**  \n",
    "  Segmente un texte en phrases (heuristique) pour prÃ©parer un chunking â€œphrase-awareâ€.\n",
    "  - **Concepts** : phrase-aware, segmentation.\n",
    "\n",
    "- **`split_long_text_phrase_aware(text, max_chars=1200)`**  \n",
    "  DÃ©coupe un texte long en sous-parties proches de `max_chars` en privilÃ©giant les frontiÃ¨res de phrases.\n",
    "  - **Concepts** : `max_chars`, chunking contrÃ´lÃ©, qualitÃ© de lecture/citation.\n",
    "\n",
    "- **`extract_article_meta(root, xml_path)`**  \n",
    "  Extrait des mÃ©tadonnÃ©es dâ€™article (ex: numÃ©ro, titre, identifiants) pour tracer et Ã©valuer la pertinence plus proprement.\n",
    "  - **Concepts** : `meta.num` (numÃ©ro dâ€™article), traÃ§abilitÃ©, Ã©valuation â€œarticle-awareâ€.\n",
    "\n",
    "- **`extract_links(root, max_links=20)`**  \n",
    "  Collecte un Ã©chantillon de liens (audit) et les stocke en mÃ©tadonnÃ©es pour inspection/filtrage Ã©ventuel.\n",
    "  - **Concepts** : audit de corpus, liens comme bruit/indice.\n",
    "\n",
    "- **`_walk_collect_paragraphs(node, paragraphs)`**  \n",
    "  Parcourt lâ€™arbre XML et accumule des paragraphes textuels utiles en Ã©vitant les zones peu pertinentes.\n",
    "  - **Concepts** : extraction structurÃ©e, sÃ©lection de contenu.\n",
    "\n",
    "- **`extract_paragraphs(xml_path)`**  \n",
    "  Extrait une liste de paragraphes â€œpropresâ€ depuis un XML (cÅ“ur du passage-level : on part de `<p>` et assimilÃ©s).\n",
    "  - **Concepts** : passage-level, granularitÃ©, rÃ©duction de bruit.\n",
    "\n",
    "- **`chunk_paragraphs(paragraphs, max_chars=1200, overlap_paragraphs=1)`**  \n",
    "  Regroupe des paragraphes en chunks passage-level avec une taille cible et un lÃ©ger overlap pour prÃ©server le contexte.\n",
    "  - **Concepts** : `max_chars`, `overlap_paragraphs`, contexte vs prÃ©cision.\n",
    "\n",
    "- **`stable_chunk_id(doc_id, chunk_index, chunk_text)`**  \n",
    "  GÃ©nÃ¨re un identifiant de chunk stable (reproductible) qui reflÃ¨te le contenu rÃ©ellement indexÃ©.\n",
    "  - **Concepts** : stabilitÃ©, dÃ©duplication, traÃ§abilitÃ©.\n",
    "\n",
    "- **`iter_xml_files(data_root)`**  \n",
    "  ItÃ¨re sur les XML dâ€™un rÃ©pertoire racine (traversal) pour batcher le traitement.\n",
    "  - **Concepts** : pipeline corpus, scalabilitÃ©.\n",
    "\n",
    "- **`build_chunk_corpus_for_file(xml_path, cfg)`**  \n",
    "  Construit les enregistrements chunkÃ©s (dicts) pour un fichier : extraction â†’ nettoyage â†’ meta â†’ chunking.\n",
    "  - **Concepts** : transformation â€œend-to-endâ€ par document.\n",
    "\n",
    "- **`build_chunk_corpus(data_root, cfg)`**  \n",
    "  Applique le chunking sur tout le corpus et agrÃ¨ge tous les chunks dans une liste dâ€™enregistrements JSONL.\n",
    "  - **Concepts** : dataset passage-level, reproductibilitÃ©.\n",
    "\n",
    "- **`save_jsonl(records, out_path)`**  \n",
    "  Sauvegarde les chunks au format JSONL (1 chunk = 1 ligne) pour indexation et benchmarks ultÃ©rieurs.\n",
    "  - **Concepts** : JSONL, ingestion simple, compatibilitÃ© tooling.\n",
    "\n",
    "- **`print_quick_stats(records)`**  \n",
    "  Affiche des stats rapides (nb docs, nb chunks, longueurs min/moy/max) pour vÃ©rifier lâ€™effet du chunking.\n",
    "  - **Concepts** : contrÃ´le qualitÃ©, dÃ©rive de corpus.\n",
    "\n",
    "- **`default_run_config()`**  \n",
    "  Fournit une config par dÃ©faut (seuils de chunking, overlap, filtres de qualitÃ©, options liens) pour runs reproductibles.\n",
    "  - **Concepts** : config explicite, reproductibilitÃ©.\n",
    "\n",
    "- **`parse_args(argv=None)`**  \n",
    "  Parse les arguments CLI (data_root, out_jsonl, max_chars, overlap, min_text_len, liens, limit_files).\n",
    "  - **Concepts** : CLI, expÃ©rimentation contrÃ´lÃ©e.\n",
    "\n",
    "- **`run_with_config(cfg)`**  \n",
    "  Lance un run complet avec une config : build corpus chunkÃ© â†’ sauvegarde â†’ stats/audit.\n",
    "  - **Concepts** : pipeline exÃ©cutable, run dÃ©terministe.\n",
    "\n",
    "- **`main(argv=None)`**  \n",
    "  Point dâ€™entrÃ©e : mode â€œconfig par dÃ©fautâ€ (sans args) ou mode CLI explicite.\n",
    "  - **Concepts** : ergonomie dev, exÃ©cution sÃ©curisÃ©e.\n",
    "\n",
    "### 3) ğŸ” Analyse de fin de script\n",
    "- **Objectif atteint** : convertir des XML juridiques en un dataset **passage-level JSONL** permettant un retrieval plus prÃ©cis (top-k = extraits directement citables) et une Ã©valuation plus robuste (pertinence au niveau passage, pas document entier).\n",
    "- **RÃ©sultats de run (ordre de grandeur)** : **~4 091 documents** sources â†’ **~13 180 chunks** ; extraction de liens en mÃ©tadonnÃ©es (audit) avec **~26,8 liens/chunk** en moyenne (utile pour dÃ©cider dâ€™un filtrage ultÃ©rieur).\n",
    "- **ContrÃ´les qualitatifs rÃ©alisÃ©s** : inspection de cas informatifs (chunk le plus long, chunk trÃ¨s court, prÃ©sence de headers techniques, liens, cohÃ©rence de segmentation) pour valider que le chunking produit du texte â€œretrievableâ€.\n",
    "- **Pourquoi câ€™est clÃ© pour la mission** : un RAG â€œproductionâ€ ne se joue pas seulement sur le modÃ¨le ; il faut un corpus **structurÃ©, traÃ§able, filtrable** pour (1) amÃ©liorer la pertinence, (2) faciliter lâ€™annotation/guide dâ€™Ã©valuation, (3) rendre lâ€™amÃ©lioration chiffrable et explicable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd45635",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ§ª <span style=\"color:green; font-weight:700;\">Script 9</span> â€” <span style=\"color:orange; font-weight:800;\">9_benchmark_bm25_on_jsonl_chunks.py</span>\n",
    "\n",
    "### 1) Phrase dâ€™introduction\n",
    "> ğŸ¯ **Â« Je benchmarke BM25 sur un corpus *passage-level* (chunks JSONL issus du script 8) afin de mesurer, de faÃ§on chiffrÃ©e et audit-able (mÃ©triques + verbatim), lâ€™impact du chunking et dâ€™amÃ©liorations â€œpetites mais explicablesâ€ (query understanding et expansion mÃ©tier minimale) sur la chaÃ®ne â€œquestion utilisateur â†’ bons extraitsâ€. Â»**\n",
    "\n",
    "### 2) âš™ï¸ Fonctions\n",
    "- **`tokenize(text)`**  \n",
    "  Tokenisation simple (minuscules + mots alphanumÃ©riques) pour un BM25 lisible et â€œdebuggableâ€.\n",
    "  - **Concepts Ã  expliciter** : BM25 = lexical, tokenisation baseline, reproductibilitÃ©.\n",
    "\n",
    "- **`expand_query_minimal_legal(query)`**  \n",
    "  Ajoute une **expansion mÃ©tier minimale** (heuristique) Ã  la requÃªte pour rÃ©duire les Ã©checs BM25 dus au mismatch lexical, sans complexifier (objectif : petit / explicable).\n",
    "  - **Concepts Ã  expliciter** : expansion contrÃ´lÃ©e, mismatch lexical, amÃ©lioration â€œcheap but effectiveâ€.\n",
    "\n",
    "- **`get_dedupe_value(doc, dedupe_key)`**  \n",
    "  Extrait une valeur stable (ex: `meta.num`, `meta.id`, `doc_id`) servant Ã  **dÃ©dupliquer** un ranking.\n",
    "  - **Concepts Ã  expliciter** : dÃ©duplication = Ã©viter 10 variantes du mÃªme article, `dedupe_key` = clÃ© de regroupement.\n",
    "\n",
    "- **`dedupe_ranked_results(ranked_results, dedupe_key)`**  \n",
    "  DÃ©duplique une liste triÃ©e `(doc, score)` en conservant le **premier** document pour chaque valeur de `dedupe_key`.\n",
    "  - **Concepts Ã  expliciter** : ranking triÃ©, dÃ©duplication â€œfirst winsâ€, lisibilitÃ© du top-k.\n",
    "\n",
    "- **`bm25_search(query, bm25, documents, top_k)`**  \n",
    "  Retourne le top-k `(doc, score)` selon BM25 (scoring + tri dÃ©croissant).\n",
    "  - **Concepts Ã  expliciter** : score BM25, ranking, top-k, baseline explicable.\n",
    "\n",
    "- **`is_relevant_by_keywords(text, relevant_keywords)`**  \n",
    "  ImplÃ©mente lâ€™**oracle V1** : un document/chunk est affichÃ© **PERTINENT** sâ€™il contient au moins un mot-clÃ© attendu (`relevant_keywords`).\n",
    "  - **Concepts Ã  expliciter** : oracle/ground truth du benchmark, â€œpertinentâ€ = critÃ¨re binaire, faux positif / faux nÃ©gatif.\n",
    "\n",
    "- **`evaluate_bm25(queries, documents, bm25, top_k, use_query_understanding, use_minimal_expansion, dedupe_key)`**  \n",
    "  ExÃ©cute le benchmark et calcule **Recall@k, MRR, nDCG@k** en testant 4 modes : requÃªte brute/enrichie Ã— avec/sans expansion minimale, avec option de dÃ©duplication.\n",
    "  - **Concepts Ã  expliciter** : protocole constant, comparaison â€œun seul changementâ€, mÃ©triques de retrieval (avant gÃ©nÃ©ration).\n",
    "\n",
    "- **`display_results_per_query(queries, documents, bm25, top_k, use_query_understanding, use_minimal_expansion, dedupe_key)`**  \n",
    "  Affiche le **verbatim par question** : top-k + scores + drapeau **PERTINENT** (oracle), avec dÃ©duplication optionnelle.\n",
    "  - **Concepts Ã  expliciter** : audit qualitatif, interprÃ©tation des mÃ©triques, diagnostic des erreurs.\n",
    "\n",
    "- **`parse_args(argv=None)`**  \n",
    "  Parse les arguments CLI et utilise `parse_known_args()` pour rester robuste en environnement type Spyder/IPython.\n",
    "  - **Concepts Ã  expliciter** : robustesse tooling, exÃ©cution reproductible.\n",
    "\n",
    "- **`parse_csv_list(s)`**  \n",
    "  Parse une liste CSV (ex: `\"article,section_ta\"`) en nettoyant les espaces.\n",
    "  - **Concepts Ã  expliciter** : ergonomie CLI, paramÃ¨tres contrÃ´lÃ©s.\n",
    "\n",
    "- **`load_jsonl_documents_robust(path, **kwargs)`**  \n",
    "  Charge les chunks JSONL via `corpus_loader_jsonl.load_documents` en gÃ©rant plusieurs signatures possibles (robustesse si le loader Ã©volue).\n",
    "  - **Concepts Ã  expliciter** : compatibilitÃ©, stabilitÃ© du pipeline, dette technique maÃ®trisÃ©e.\n",
    "\n",
    "- **`main()`**  \n",
    "  Orchestration : charge corpus JSONL chunkÃ©, prÃ©pare BM25, lance lâ€™Ã©valuation + verbatim selon les options CLI.\n",
    "  - **Concepts Ã  expliciter** : pipeline â€œend-to-endâ€ benchmarkable.\n",
    "\n",
    "### 3) ğŸ” Analyse de fin de script\n",
    "- **Protocole** : benchmark BM25 sur **chunks JSONL** (passage-level) avec :\n",
    "  - mÃ©triques **Recall@10 / MRR / nDCG@10**,\n",
    "  - **verbatim top-k** pour expliquer les chiffres,\n",
    "  - options : requÃªte enrichie (query understanding) et/ou **expansion mÃ©tier minimale**, et dÃ©duplication du top-k (`dedupe_key`).\n",
    "\n",
    "- **RÃ©sultats (tels que synthÃ©tisÃ©s dans le script)** :\n",
    "  - **BM25 chunks â€” requÃªte brute** : Recall@10 **0,667** ; MRR **0,444** ; nDCG@10 **0,500**\n",
    "  - **BM25 chunks â€” requÃªte enrichie** : Recall@10 **0,667** ; MRR **0,444** ; nDCG@10 **0,500**\n",
    "  - **BM25 chunks â€” brute + expansion minimale** : Recall@10 **0,667** ; MRR **0,667** ; nDCG@10 **0,667**\n",
    "  - **BM25 chunks â€” enrichie + expansion minimale** : Recall@10 **0,667** ; MRR **0,667** ; nDCG@10 **0,667**\n",
    "\n",
    "- **InterprÃ©tation (quantitatif)** :\n",
    "  - Le **Recall@10 reste stable** (2 questions sur 3 â€œhitâ€ dans le top-10) : le chunking aide mais BM25 reste limitÃ© sur certaines requÃªtes.\n",
    "  - Lâ€™**expansion mÃ©tier minimale** amÃ©liore fortement le **ranking** :\n",
    "    - **MRR** augmente (le premier pertinent remonte souvent au rang 1),\n",
    "    - **nDCG@10** augmente (top-10 mieux ordonnÃ©, pertinents plus haut).\n",
    "\n",
    "- **InterprÃ©tation (qualitatif via verbatim)** :\n",
    "  - Sur les requÃªtes â€œlexicalisÃ©esâ€, BM25 sur chunks fonctionne bien.\n",
    "  - Lâ€™Ã©chec persistant sur une requÃªte plus â€œsÃ©mantiqueâ€ illustre une limite attendue : BM25 remonte des passages proches lexicalement (structure, termes gÃ©nÃ©riques) sans tomber sur lâ€™extrait normatif prÃ©cis.\n",
    "\n",
    "- **Ce quâ€™il faut valoriser en entretien (alignÃ© mission)** :\n",
    "  1) Passage-level + benchmark = retrieval **mesurable, stable, amÃ©liorable** (on prouve les gains par mÃ©triques + audit).  \n",
    "  2) Une amÃ©lioration **simple, explicable et versionnable** (expansion minimale) peut donner un gain net de ranking sans changer de modÃ¨le.  \n",
    "  3) Les Ã©checs restants deviennent des signaux exploitables : ils justifient lâ€™Ã©tape suivante (dense/hybride ou filtrage plus fin des passages â€œplan/structureâ€) plutÃ´t que du â€œtuning au feelingâ€.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c5cec1",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ§ª <span style=\"color:green; font-weight:700;\">Script 10</span> â€” <span style=\"color:orange; font-weight:800;\">10_benchmark_hybride_rrf_bm25_dense_chunks.py</span>\n",
    "\n",
    "### 1) Phrase dâ€™introduction\n",
    "> ğŸ¯ **Â« Notre objectif est de rendre la chaÃ®ne â€œquestion utilisateur â†’ bons extraitsâ€ mesurable et amÃ©liorable : ce script benchmarke, sur un corpus chunkÃ© JSONL filtrable, trois retrievers (BM25, Dense, Hybride via RRF) avec un oracle V2 â€œarticle-awareâ€ et un verbatim alignÃ© sur lâ€™Ã©valuation, pour expliquer chaque succÃ¨s/Ã©chec sans ambiguÃ¯tÃ©. Â»**\n",
    "\n",
    "### 2) âš™ï¸ Fonctions\n",
    "- **`load_chunks_jsonl(jsonl_path, limit)`**  \n",
    "  Charge le corpus chunkÃ© JSONL (1 ligne = 1 chunk dict) avec option de limite.\n",
    "  - **Concepts** : JSONL passage-level, chunk = unitÃ© de retrieval/citation.\n",
    "\n",
    "- **`chunk_matches_title(chunk, needle)`**  \n",
    "  Teste si un chunk matche un critÃ¨re de titre (via `meta.titre`, sinon fallback `doc_id`).\n",
    "  - **Concepts** : filtre â€œtitle_containsâ€, robustesse si meta partielle.\n",
    "\n",
    "- **`filter_chunks(chunks, allowed_doc_types, title_contains, collection_contains)`**  \n",
    "  Filtre le corpus par `doc_type`, par sous-chaÃ®ne dans titre/doc_id, et par identifiant de collection dans `doc_id`.\n",
    "  - **Concepts** : pÃ©rimÃ¨tre contrÃ´lÃ©, rÃ©duction de bruit, reproductibilitÃ© des tests.\n",
    "\n",
    "- **`normalize_for_bm25(text)`**  \n",
    "  Normalise le texte (minuscule/espaces) pour stabiliser BM25.\n",
    "  - **Concepts** : normalisation, stabilitÃ© du scoring lexical.\n",
    "\n",
    "- **`tokenize(text)`**  \n",
    "  Tokenisation BM25 robuste (liste de tokens) adaptÃ©e Ã  un corpus juridique bruitÃ©.\n",
    "  - **Concepts** : lexical pur, observabilitÃ©/debug.\n",
    "\n",
    "- **`build_bm25_index(documents)`**  \n",
    "  Construit lâ€™index BM25 sur `documents[i][\"text\"]`.\n",
    "  - **Concepts** : baseline explicable, indexation.\n",
    "\n",
    "- **`bm25_search(query, documents, bm25, top_k)`**  \n",
    "  Retourne la liste triÃ©e `(doc_index, score)` BM25 dans le top-k.\n",
    "  - **Concepts** : ranking, top-k, score BM25.\n",
    "\n",
    "- **`_cache_key()`**  \n",
    "  Fabrique une clÃ© de cache **stable** (incluant les paramÃ¨tres de filtrage) pour Ã©viter un mauvais rÃ©-emploi dâ€™embeddings.\n",
    "  - **Concepts** : anti-dÃ©salignement, cache â€œcohÃ©rent avec le datasetâ€.\n",
    "\n",
    "- **`build_dense_embeddings(documents, model_name, cache_dir, cache_tag)`**  \n",
    "  Construit ou recharge la matrice dâ€™embeddings des chunks (cache disque).\n",
    "  - **Concepts** : performance, reproductibilitÃ©, embeddings Sentence-Transformers.\n",
    "\n",
    "- **`dense_search_all(query, model, doc_emb, top_k)`**  \n",
    "  Dense search sur **tout** le corpus (cosine similarity) et renvoie le top-k indices.\n",
    "  - **Concepts** : similaritÃ© cosinus, retrieval sÃ©mantique global.\n",
    "\n",
    "- **`dense_rerank_subset(query, model, doc_emb, subset_indices, top_k)`**  \n",
    "  Rerank dense sur un **sous-ensemble** (utile en hybride : reranker seulement une shortlist).\n",
    "  - **Concepts** : rerank, shortlist, efficacitÃ© en production.\n",
    "\n",
    "- **`hybrid_rrf_search(query, documents, bm25, model, doc_emb, top_k, shortlist_k, rrf_k)`**  \n",
    "  Hybride BM25 + Dense via **RRF (Reciprocal Rank Fusion)** : fusionne deux rankings en pondÃ©rant par le rang.\n",
    "  - **Concepts** : RRF, fusion de signaux lexical+sÃ©mantique, `shortlist_k` (taille shortlist), `rrf_k` (constante RRF).\n",
    "\n",
    "- **`minimal_business_expansion(query)`**  \n",
    "  Expansion â€œmÃ©tierâ€ ultra-minimale (heuristique) indÃ©pendante du dictionnaire, pour aider le retrieval sans boÃ®te noire.\n",
    "  - **Concepts** : amÃ©lioration simple/explicable, rÃ©duction du mismatch lexical.\n",
    "\n",
    "- **`_safe_lower(x)`**  \n",
    "  Lowercase sÃ»r (gÃ¨re `None`) pour Ã©viter des erreurs lors des accÃ¨s meta/texte.\n",
    "  - **Concepts** : robustesse.\n",
    "\n",
    "- **`_q_get(q, key, default)`**, **`q_id(q)`**, **`q_question(q)`**, **`q_num_prefixes(q)`**, **`q_keywords_fallback(q)`**  \n",
    "  AccÃ¨s robuste aux champs dâ€™une query (ID, texte, prÃ©fixes attendus, fallback mots-clÃ©s).\n",
    "  - **Concepts** : stabilitÃ© du benchmark, compatibilitÃ© V1/V2.\n",
    "\n",
    "- **`is_relevant_v2(chunk, q)`**  \n",
    "  **Oracle V2** : un chunk est **PERTINENT** si `meta.num` est prÃ©sent et matche un des `relevant_num_prefixes` attendus (sinon fallback possible).\n",
    "  - **Concepts** : oracle/ground truth, â€œarticle-awareâ€, pertinence binaire explicable.\n",
    "\n",
    "- **`recall_at_k_v2(ranked_indices, documents, q, k)`**  \n",
    "  Recall@k (V2) : 1 si au moins un chunk pertinent est dans le top-k, sinon 0.\n",
    "  - **Concepts** : â€œretrouve / ne retrouve pasâ€.\n",
    "\n",
    "- **`reciprocal_rank_v2(ranked_indices, documents, q)`**  \n",
    "  MRR (V2) : 1/rang du premier pertinent (0 si aucun).\n",
    "  - **Concepts** : â€œpremier pertinentâ€, ranking.\n",
    "\n",
    "- **`ndcg_at_k_v2(ranked_indices, documents, q, k)`**  \n",
    "  nDCG@k (V2) en pertinence binaire : qualitÃ© globale du classement.\n",
    "  - **Concepts** : ranking global, pertinents plus haut = meilleur score.\n",
    "\n",
    "- **`get_query_variant(q, dictionary, use_query_understanding, use_min_expansion)`**  \n",
    "  Construit la requÃªte effective envoyÃ©e au retriever : brute / enrichie via dictionnaire / + expansion minimale.\n",
    "  - **Concepts** : â€œrequÃªte rÃ©ellement testÃ©eâ€, auditabilitÃ©, alignement expÃ©rimental.\n",
    "\n",
    "- **`evaluate_retriever(name, queries, documents, retriever_fn, top_k, metrics_k)`**  \n",
    "  Calcule Recall@k, MRR, nDCG@k sur toutes les queries pour un retriever donnÃ© (BM25, Dense, Hybride).\n",
    "  - **Concepts** : protocole constant, comparaison mesurÃ©e (pas au feeling).\n",
    "\n",
    "- **`show_verbatim(queries, documents, retriever_fn, top_k, label)`**  \n",
    "  Affiche le top-k par requÃªte avec marquage **PERTINENT** (oracle V2) : audit qualitatif et diagnostic dâ€™erreurs.\n",
    "  - **Concepts** : verbatim alignÃ© sur lâ€™Ã©valuation, explicabilitÃ©.\n",
    "\n",
    "- **`parse_args()`**  \n",
    "  Parse les options CLI (filtres `doc_types`, `collection_contains`, `title_contains`, tailles `top_k`, `bm25_shortlist`, etc.).\n",
    "  - **Concepts** : runs reproductibles, paramÃ¨tres contrÃ´lÃ©s.\n",
    "\n",
    "- **`main()`**  \n",
    "  Orchestration complÃ¨te : chargement + filtrage chunks â†’ BM25 â†’ embeddings (cache) â†’ benchmarks BM25/Dense/Hybride + verbatim.\n",
    "\n",
    "### 3) ğŸ” Analyse de fin de script\n",
    "- **Ce que ce script verrouille (qualitÃ© dâ€™expÃ©rimentation)** :\n",
    "  - **Oracle V2 cohÃ©rent** (pertinence par `meta.num` + prÃ©fixes attendus) â†’ mesure plus â€œjuridiqueâ€ que du simple mot-clÃ©.\n",
    "  - **Verbatim alignÃ©** : ce qui est Ã©valuÃ© est exactement ce qui est affichÃ© (pas dâ€™artefact dâ€™affichage).\n",
    "  - **Cache embeddings anti-dÃ©salignement** : les embeddings correspondent bien au corpus filtrÃ© utilisÃ© au benchmark.\n",
    "\n",
    "- **Ce que le dernier run montre (lecture produit, pas â€œbugâ€)** :\n",
    "  - Sur le pÃ©rimÃ¨tre filtrÃ© (exemple : ~386 chunks â€œarticleâ€ dâ€™une collection), une requÃªte peut rÃ©ussir nettement (ex : dÃ©finition du licenciement Ã©conomique) tandis que dâ€™autres restent en Ã©chec (ex : â€œsans prÃ©avisâ€, â€œcontesterâ€) â†’ ce nâ€™est plus un problÃ¨me de mÃ©triques, mais un problÃ¨me de **signal retrieval / formulation / pÃ©rimÃ¨tre**.\n",
    "\n",
    "- **InterprÃ©tation utile pour la mission (â€œquestion â†’ bons extraitsâ€)** :\n",
    "  - Les Ã©checs restants sont dÃ©sormais **diagnosticables** : soit le signal lexical+sÃ©mantique â€œlicenciementâ€ attire vers un mauvais chapitre, soit les formulations attendues sont dispersÃ©es/hors pÃ©rimÃ¨tre, soit lâ€™enrichissement actuel nâ€™est pas assez discriminant vis-Ã -vis de lâ€™oracle V2.\n",
    "  - 1 seul dataset xml Ã©tant testÃ© sur des centaines, il se peut aussi que notre corpus de test soit trop limitÃ© ;-)\n",
    "  - Prochaine marche â€œproduction-gradeâ€ (sans relancer des tests ici) : **routing/contraintes structurelles** (par typologie/chapitre), enrichissement mÃ©tier plus ciblÃ©, et/ou reranking plus fort sur shortlist (dense rerank) â€” toujours avec mÃ©triques + verbatim.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML-Pro-venv)",
   "language": "python",
   "name": "ml-pro-venv"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

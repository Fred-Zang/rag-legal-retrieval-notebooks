{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e3e07b6",
   "metadata": {},
   "source": [
    "# üß≠ Script 06 ‚Äî BM25 filtr√© + Query Understanding (Notebook de pr√©sentation)\n",
    "\n",
    "## üß© Pourquoi cette progression ?\n",
    "Dans les scripts pr√©c√©dents, nous avons √©tabli une **baseline BM25** puis test√© deux leviers classiques d‚Äôam√©lioration :\n",
    "\n",
    "- **Filtrage m√©tier** (r√©duire le bruit documentaire en limitant le p√©rim√®tre au *Code du travail*).\n",
    "- **Query Understanding** (tenter d‚Äô**enrichir** la requ√™te utilisateur avec des termes ‚Äúm√©tier‚Äù issus d‚Äôun dictionnaire).\n",
    "\n",
    "L‚Äôid√©e de ce script est de **combiner** ces deux leviers :\n",
    "1) on restreint le corpus √† un p√©rim√®tre juridique coh√©rent (*Code du travail*),  \n",
    "2) puis on applique l‚Äôenrichissement de requ√™te **avant** le retrieval BM25,  \n",
    "3) et on r√©-√©value **sur le m√™me benchmark** avec **les m√™mes m√©triques** (Recall@10, MRR, nDCG@10).\n",
    "\n",
    "‚úÖ Si les m√©triques bougent, c‚Äôest attribuable √† cette combinaison (filtrage + enrichissement), √† p√©rim√®tre et protocole constants.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a9fe24",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Pr√©-requis (Notebook)\n",
    "Ce notebook ajoute uniquement des cellules ‚Äúsetup‚Äù (installation + import du projet) pour √©viter les erreurs dans Jupyter.\n",
    "\n",
    "- Les modules `corpus_loader`, `benchmark_queries`, `metrics`, `query_understanding` sont **locaux au projet** : il faut que la racine de notre projet soit dans `sys.path`.\n",
    "- D√©pendances externes attendues : `rank-bm25` (et souvent `pyyaml` si le dictionnaire est en YAML).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcf0aeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Installation des d√©pendances externes (√† ex√©cuter une seule fois par environnement)\n",
    "# Note: dans Jupyter, %pip installe dans le kernel courant.\n",
    "%pip install -q rank-bm25 pyyaml ipywidgets tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a7c76ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: d:\\-- Projet RAG Avocats --\\codes_python\\notebooks\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Rendre les modules locaux importables (ex: corpus_loader.py, metrics.py, etc.)\n",
    "# Si notre notebook est dans un sous-dossier (ex: notebooks/), remonter d'un ou plusieurs niveaux.\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "# Exemple si le notebook est dans un dossier notebooks/ :\n",
    "# PROJECT_ROOT = Path.cwd().parent\n",
    "\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e392120d",
   "metadata": {},
   "source": [
    "## üßæ Code du script (inchang√©)\n",
    "\n",
    "> Le code ci-dessous est repris tel quel du fichier `.py` (hors cellules setup notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4f9aeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus brut charg√© : 4422 documents\n",
      "Corpus filtr√© 'Code du travail' : 882 documents\n",
      "\n",
      "=== BM25 filtr√© + requ√™te enrichie ===\n",
      "{'Recall@10': 0.3333333333333333, 'MRR': 0.3333333333333333, 'nDCG@10': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "STAGE 8 ‚Äì BM25 + FILTRAGE M√âTIER + QUERY UNDERSTANDING\n",
    "\"\"\"\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from corpus_loader import documents\n",
    "from query_understanding import process_user_query, load_juridical_dictionary\n",
    "from benchmark_queries import benchmark_queries\n",
    "from metrics import recall_at_k, reciprocal_rank, ndcg_at_k\n",
    "import re\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 0. CHARGEMENT DU DICO DE juridical_dictionary.yml\n",
    "# =========================================================\n",
    "dictionary = load_juridical_dictionary(\"juridical_dictionary.yml\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1. FILTRAGE M√âTIER (CODE DU TRAVAIL)\n",
    "# =========================================================\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "\n",
    "\n",
    "filtered_documents = [\n",
    "    doc for doc in documents\n",
    "    if \"code du travail\" in doc[\"text\"].lower()\n",
    "]\n",
    "\n",
    "tokenized_docs = [tokenize(doc[\"text\"]) for doc in filtered_documents]\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "\n",
    "def bm25_search(query, k=10):\n",
    "    scores = bm25.get_scores(tokenize(query))\n",
    "    ranked = sorted(\n",
    "        zip(filtered_documents, scores),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "    return ranked[:k]\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. BENCHMARK\n",
    "# =========================================================\n",
    "\n",
    "def evaluate():\n",
    "    recall_scores, mrr_scores, ndcg_scores = [], [], []\n",
    "\n",
    "    for q in benchmark_queries:\n",
    "        enriched = process_user_query(q[\"question\"], dictionary)\n",
    "        query = enriched[\"enriched_query\"]\n",
    "\n",
    "        results = bm25_search(query, k=10)\n",
    "\n",
    "        recall_scores.append(recall_at_k(results, q[\"relevant_keywords\"], 10))\n",
    "        mrr_scores.append(reciprocal_rank(results, q[\"relevant_keywords\"]))\n",
    "        ndcg_scores.append(ndcg_at_k(results, q[\"relevant_keywords\"], 10))\n",
    "\n",
    "    return {\n",
    "        \"Recall@10\": sum(recall_scores) / len(recall_scores),\n",
    "        \"MRR\": sum(mrr_scores) / len(mrr_scores),\n",
    "        \"nDCG@10\": sum(ndcg_scores) / len(ndcg_scores),\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n=== BM25 filtr√© + requ√™te enrichie ===\")\n",
    "    print(evaluate())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cb4f18",
   "metadata": {},
   "source": [
    "## üìä Analyse finale (comparative)\n",
    "\n",
    "R√©sultat observ√© lors du run :\n",
    "\n",
    "- Corpus brut charg√© : **4422** documents  \n",
    "- Corpus filtr√© **¬´ Code du travail ¬ª** : **882** documents  \n",
    "- **BM25 filtr√© + requ√™te enrichie** :  \n",
    "  - Recall@10 = **0.333**\n",
    "  - MRR = **0.333**\n",
    "  - nDCG@10 = **0.333**\n",
    "\n",
    "### üîç Lecture\n",
    "- Le score est **identique** √† la baseline observ√©e dans les stages BM25 pr√©c√©dents.\n",
    "- Dans notre cas, cela s‚Äôexplique tr√®s souvent par un point d√©j√† identifi√© :  \n",
    "  si `process_user_query()` ne d√©tecte **pas d‚Äôintention m√©tier** (`intent_detected=None`), alors `enriched_query == query` ‚Üí **aucune variation possible** c√¥t√© BM25.\n",
    "\n",
    "### ‚úÖ Conclusion\n",
    "Ce stage est surtout utile pour **valider l‚Äôarchitecture exp√©rimentale** :  \n",
    "filtrage + enrichissement sont ‚Äúbranch√©s‚Äù, et on peut d√©sormais mesurer un gain **d√®s que** :\n",
    "- la d√©tection d‚Äôintention se d√©clenche (dictionnaire plus robuste / matching moins strict),\n",
    "- ou que l‚Äôon passe √† une approche hybride (BM25 + dense) / re-ranking.\n",
    "\n",
    "> √Ä ce stade, l‚Äôabsence de gain ne signifie pas que l‚Äôid√©e est mauvaise : elle indique que l‚Äôenrichissement *n‚Äôa pas eu d‚Äôeffet* sur le benchmark actuel (souvent parce qu‚Äôil ne s‚Äôactive pas).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2836b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BM25 filtr√© + requ√™te enrichie (re-run notebook) ===\n",
      "{'Recall@10': 0.3333333333333333, 'MRR': 0.3333333333333333, 'nDCG@10': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "# üîÅ Optionnel : relancer l'√©valuation ici pour v√©rifier les scores dans notre environnement notebook\n",
    "# (Si tout est bien install√©/import√©, cela doit reproduire les r√©sultats.)\n",
    "try:\n",
    "    print(\"\\n=== BM25 filtr√© + requ√™te enrichie (re-run notebook) ===\")\n",
    "    print(evaluate())\n",
    "except Exception as e:\n",
    "    print(\"Erreur lors du re-run notebook:\", repr(e))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML-Pro-venv)",
   "language": "python",
   "name": "ml-pro-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
